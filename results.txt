2025-12-13 07:16:42.508709: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-13 07:16:42.525963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1765610202.546891    3035 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1765610202.553309    3035 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1765610202.569239    3035 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765610202.569265    3035 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765610202.569267    3035 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765610202.569270    3035 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-13 07:16:42.574087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
LOADING DATA
======================================================================

Loading 100 training passages...
Loading WikiText-103 (train split)...
Note: This may take a few minutes to download (~300MB). Use use_small=True for faster testing.
Loaded 100 text samples
Average length: 648.6 characters
Loaded 100 training passages
Loading 100 novel passages from test set...
Loading WikiText-103 (test split)...
Note: This may take a few minutes to download (~300MB). Use use_small=True for faster testing.
Loaded 100 text samples
Average length: 731.0 characters
Loaded 100 novel passages

Using device: cuda
GPU: NVIDIA L4
Memory: 23.80 GB

======================================================================
CHECKPOINT: 1 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 1 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 1, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
  0% 0/25 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
{'loss': 4.0557, 'grad_norm': 11.648784637451172, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.4}
{'loss': 3.9491, 'grad_norm': 13.972878456115723, 'learning_rate': 1.2e-05, 'epoch': 0.8}
{'train_runtime': 2.8081, 'train_samples_per_second': 35.611, 'train_steps_per_second': 8.903, 'train_loss': 3.9696797943115234, 'epoch': 1.0}
100% 25/25 [00:02<00:00,  8.90it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics:   0% 0/200 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Computing reproduction metrics: 100% 200/200 [03:26<00:00,  1.03s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:04<00:00, 45.90it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.2713
  Std compression: 1.8353

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.8063
  Std compression: 1.6048

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.2617
  Std compression: 1.5633

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.6335
  Std compression: 1.4437

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.9732
  Std compression: 1.4039

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.2404
  Std compression: 1.4371

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.5846
  Std compression: 1.4157

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.9560
  Std compression: 1.4273

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.3781
  Std compression: 1.4597

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.8386
  Std compression: 1.4503

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.0215
  Std compression: 1.7179

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = 0.0101, p = 8.8707e-01
  Memorized sequences: mean = -5.2271, std = 0.2167, n = 100
  Novel sequences: mean = -5.2313, std = 0.1964, n = 100
  Difference: 0.0042
  T-test: t = 0.1422, p = 8.8709e-01
  ✗ WEAK: No significant correlation
Layer 0: r=0.010, p=8.87e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = 0.0223, p = 7.5387e-01
  Memorized sequences: mean = -5.7646, std = 0.2082, n = 100
  Novel sequences: mean = -5.7731, std = 0.1739, n = 100
  Difference: 0.0086
  T-test: t = 0.3139, p = 7.5393e-01
  ✗ WEAK: No significant correlation
Layer 1: r=0.022, p=7.54e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = 0.0156, p = 8.2606e-01
  Memorized sequences: mean = -6.2180, std = 0.2220, n = 100
  Novel sequences: mean = -6.2242, std = 0.1757, n = 100
  Difference: 0.0063
  T-test: t = 0.2201, p = 8.2605e-01
  ✗ WEAK: No significant correlation
Layer 2: r=0.016, p=8.26e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = 0.0125, p = 8.6070e-01
  Memorized sequences: mean = -6.5938, std = 0.2104, n = 100
  Novel sequences: mean = -6.5985, std = 0.1616, n = 100
  Difference: 0.0047
  T-test: t = 0.1757, p = 8.6071e-01
  ✗ WEAK: No significant correlation
Layer 3: r=0.012, p=8.61e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = 0.0414, p = 5.6016e-01
  Memorized sequences: mean = -6.9226, std = 0.2129, n = 100
  Novel sequences: mean = -6.9381, std = 0.1570, n = 100
  Difference: 0.0155
  T-test: t = 0.5836, p = 5.6016e-01
  ✗ WEAK: No significant correlation
Layer 4: r=0.041, p=5.60e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = 0.0426, p = 5.4920e-01
  Memorized sequences: mean = -7.1890, std = 0.2142, n = 100
  Novel sequences: mean = -7.2051, std = 0.1596, n = 100
  Difference: 0.0161
  T-test: t = 0.6000, p = 5.4920e-01
  ✗ WEAK: No significant correlation
Layer 5: r=0.043, p=5.49e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = 0.0527, p = 4.5899e-01
  Memorized sequences: mean = -7.5281, std = 0.2211, n = 100
  Novel sequences: mean = -7.5482, std = 0.1540, n = 100
  Difference: 0.0201
  T-test: t = 0.7419, p = 4.5901e-01
  ✗ WEAK: No significant correlation
Layer 6: r=0.053, p=4.59e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = 0.0230, p = 7.4662e-01
  Memorized sequences: mean = -7.9108, std = 0.2163, n = 100
  Novel sequences: mean = -7.9194, std = 0.1536, n = 100
  Difference: 0.0086
  T-test: t = 0.3235, p = 7.4662e-01
  ✗ WEAK: No significant correlation
Layer 7: r=0.023, p=7.47e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = 0.0026, p = 9.7122e-01
  Memorized sequences: mean = -8.3371, std = 0.2276, n = 100
  Novel sequences: mean = -8.3381, std = 0.1600, n = 100
  Difference: 0.0010
  T-test: t = 0.0362, p = 9.7119e-01
  ✗ WEAK: No significant correlation
Layer 8: r=0.003, p=9.71e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.0068, p = 9.2364e-01
  Memorized sequences: mean = -8.8029, std = 0.2249, n = 100
  Novel sequences: mean = -8.8003, std = 0.1570, n = 100
  Difference: -0.0026
  T-test: t = -0.0959, p = 9.2366e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.007, p=9.24e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = 0.1002, p = 1.5811e-01
  Memorized sequences: mean = -6.9219, std = 0.2642, n = 100
  Novel sequences: mean = -6.9744, std = 0.2574, n = 100
  Difference: 0.0525
  T-test: t = 1.4168, p = 1.5811e-01
  ✗ WEAK: No significant correlation
Layer 10: r=0.100, p=1.58e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 3 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 3 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 3, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.049, 'grad_norm': 11.64043140411377, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.4}
{'loss': 3.9029, 'grad_norm': 14.509300231933594, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.8}
{'loss': 3.5465, 'grad_norm': 13.362034797668457, 'learning_rate': 3.066666666666667e-05, 'epoch': 1.2}
{'loss': 3.3793, 'grad_norm': 10.514572143554688, 'learning_rate': 2.4e-05, 'epoch': 1.6}
{'loss': 3.4113, 'grad_norm': 14.185432434082031, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}
{'loss': 3.1612, 'grad_norm': 11.000232696533203, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.4}
{'loss': 3.1602, 'grad_norm': 11.217558860778809, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.8}
{'train_runtime': 6.3762, 'train_samples_per_second': 47.05, 'train_steps_per_second': 11.762, 'train_loss': 3.487550048828125, 'epoch': 3.0}
100% 75/75 [00:06<00:00, 11.76it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:26<00:00,  1.03s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 53.09it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.2788
  Std compression: 1.8393

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.8082
  Std compression: 1.6138

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.2823
  Std compression: 1.4780

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.6441
  Std compression: 1.4184

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.9912
  Std compression: 1.4808

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.2702
  Std compression: 1.3983

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.6259
  Std compression: 1.4116

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.9924
  Std compression: 1.4330

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4061
  Std compression: 1.4039

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.8413
  Std compression: 1.4467

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.0935
  Std compression: 1.7535

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = 0.0111, p = 8.7620e-01
  Memorized sequences: mean = -5.2331, std = 0.2196, n = 100
  Novel sequences: mean = -5.2377, std = 0.1997, n = 100
  Difference: 0.0047
  T-test: t = 0.1560, p = 8.7619e-01
  ✗ WEAK: No significant correlation
Layer 0: r=0.011, p=8.76e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = 0.0137, p = 8.4718e-01
  Memorized sequences: mean = -5.7676, std = 0.2078, n = 100
  Novel sequences: mean = -5.7730, std = 0.1780, n = 100
  Difference: 0.0053
  T-test: t = 0.1929, p = 8.4720e-01
  ✗ WEAK: No significant correlation
Layer 1: r=0.014, p=8.47e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = 0.0040, p = 9.5463e-01
  Memorized sequences: mean = -6.2439, std = 0.2096, n = 100
  Novel sequences: mean = -6.2454, std = 0.1707, n = 100
  Difference: 0.0015
  T-test: t = 0.0570, p = 9.5461e-01
  ✗ WEAK: No significant correlation
Layer 2: r=0.004, p=9.55e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0030, p = 9.6643e-01
  Memorized sequences: mean = -6.6089, std = 0.2066, n = 100
  Novel sequences: mean = -6.6078, std = 0.1634, n = 100
  Difference: -0.0011
  T-test: t = -0.0422, p = 9.6639e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.003, p=9.66e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = 0.0358, p = 6.1463e-01
  Memorized sequences: mean = -6.9413, std = 0.2161, n = 100
  Novel sequences: mean = -6.9549, std = 0.1623, n = 100
  Difference: 0.0137
  T-test: t = 0.5043, p = 6.1463e-01
  ✗ WEAK: No significant correlation
Layer 4: r=0.036, p=6.15e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = 0.0320, p = 6.5317e-01
  Memorized sequences: mean = -7.2227, std = 0.2135, n = 100
  Novel sequences: mean = -7.2347, std = 0.1572, n = 100
  Difference: 0.0120
  T-test: t = 0.4500, p = 6.5318e-01
  ✗ WEAK: No significant correlation
Layer 5: r=0.032, p=6.53e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = 0.0417, p = 5.5799e-01
  Memorized sequences: mean = -7.5726, std = 0.2235, n = 100
  Novel sequences: mean = -7.5887, std = 0.1551, n = 100
  Difference: 0.0160
  T-test: t = 0.5868, p = 5.5798e-01
  ✗ WEAK: No significant correlation
Layer 6: r=0.042, p=5.58e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = 0.0106, p = 8.8116e-01
  Memorized sequences: mean = -7.9502, std = 0.2223, n = 100
  Novel sequences: mean = -7.9543, std = 0.1564, n = 100
  Difference: 0.0041
  T-test: t = 0.1497, p = 8.8116e-01
  ✗ WEAK: No significant correlation
Layer 7: r=0.011, p=8.81e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.0243, p = 7.3312e-01
  Memorized sequences: mean = -8.3731, std = 0.2186, n = 100
  Novel sequences: mean = -8.3638, std = 0.1593, n = 100
  Difference: -0.0093
  T-test: t = -0.3414, p = 7.3318e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.024, p=7.33e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.0215, p = 7.6298e-01
  Memorized sequences: mean = -8.8085, std = 0.2246, n = 100
  Novel sequences: mean = -8.8001, std = 0.1609, n = 100
  Difference: -0.0084
  T-test: t = -0.3020, p = 7.6297e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.021, p=7.63e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = 0.0677, p = 3.4068e-01
  Memorized sequences: mean = -7.0194, std = 0.2616, n = 100
  Novel sequences: mean = -7.0527, std = 0.2276, n = 100
  Difference: 0.0333
  T-test: t = 0.9551, p = 3.4067e-01
  ✗ WEAK: No significant correlation
Layer 10: r=0.068, p=3.41e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 5 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 5 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 5, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.0477, 'grad_norm': 11.634761810302734, 'learning_rate': 4.64e-05, 'epoch': 0.4}
{'loss': 3.8952, 'grad_norm': 14.599437713623047, 'learning_rate': 4.24e-05, 'epoch': 0.8}
{'loss': 3.5221, 'grad_norm': 13.29394817352295, 'learning_rate': 3.8400000000000005e-05, 'epoch': 1.2}
{'loss': 3.3268, 'grad_norm': 10.654236793518066, 'learning_rate': 3.4399999999999996e-05, 'epoch': 1.6}
{'loss': 3.3412, 'grad_norm': 14.57402515411377, 'learning_rate': 3.04e-05, 'epoch': 2.0}
{'loss': 3.0132, 'grad_norm': 11.576140403747559, 'learning_rate': 2.64e-05, 'epoch': 2.4}
{'loss': 2.9973, 'grad_norm': 11.83227252960205, 'learning_rate': 2.2400000000000002e-05, 'epoch': 2.8}
{'loss': 2.9207, 'grad_norm': 12.9548921585083, 'learning_rate': 1.84e-05, 'epoch': 3.2}
{'loss': 2.7996, 'grad_norm': 11.934592247009277, 'learning_rate': 1.44e-05, 'epoch': 3.6}
{'loss': 2.7268, 'grad_norm': 12.726608276367188, 'learning_rate': 1.04e-05, 'epoch': 4.0}
{'loss': 2.6687, 'grad_norm': 10.974896430969238, 'learning_rate': 6.4000000000000006e-06, 'epoch': 4.4}
{'loss': 2.7176, 'grad_norm': 12.827969551086426, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.8}
{'train_runtime': 10.847, 'train_samples_per_second': 46.096, 'train_steps_per_second': 11.524, 'train_loss': 3.1470342483520506, 'epoch': 5.0}
100% 125/125 [00:10<00:00, 11.52it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:28<00:00,  1.04s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 53.27it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.2641
  Std compression: 1.8429

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.8017
  Std compression: 1.5367

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.2762
  Std compression: 1.5288

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.6407
  Std compression: 1.4126

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.9915
  Std compression: 1.4231

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.2637
  Std compression: 1.4555

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.6301
  Std compression: 1.4167

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.0021
  Std compression: 1.4131

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4156
  Std compression: 1.4025

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.8451
  Std compression: 1.4523

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.1714
  Std compression: 1.7191

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = 0.0123, p = 8.6228e-01
  Memorized sequences: mean = -5.2172, std = 0.2233, n = 100
  Novel sequences: mean = -5.2224, std = 0.2007, n = 100
  Difference: 0.0052
  T-test: t = 0.1737, p = 8.6228e-01
  ✗ WEAK: No significant correlation
Layer 0: r=0.012, p=8.62e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = 0.0109, p = 8.7862e-01
  Memorized sequences: mean = -5.7633, std = 0.2004, n = 100
  Novel sequences: mean = -5.7674, std = 0.1710, n = 100
  Difference: 0.0040
  T-test: t = 0.1530, p = 8.7859e-01
  ✗ WEAK: No significant correlation
Layer 1: r=0.011, p=8.79e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = 0.0121, p = 8.6550e-01
  Memorized sequences: mean = -6.2364, std = 0.2177, n = 100
  Novel sequences: mean = -6.2411, std = 0.1707, n = 100
  Difference: 0.0047
  T-test: t = 0.1696, p = 8.6552e-01
  ✗ WEAK: No significant correlation
Layer 2: r=0.012, p=8.65e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = 0.0003, p = 9.9699e-01
  Memorized sequences: mean = -6.6048, std = 0.2099, n = 100
  Novel sequences: mean = -6.6049, std = 0.1632, n = 100
  Difference: 0.0001
  T-test: t = 0.0038, p = 9.9699e-01
  ✗ WEAK: No significant correlation
Layer 3: r=0.000, p=9.97e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = 0.0337, p = 6.3564e-01
  Memorized sequences: mean = -6.9430, std = 0.2163, n = 100
  Novel sequences: mean = -6.9558, std = 0.1568, n = 100
  Difference: 0.0127
  T-test: t = 0.4745, p = 6.3565e-01
  ✗ WEAK: No significant correlation
Layer 4: r=0.034, p=6.36e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = 0.0377, p = 5.9647e-01
  Memorized sequences: mean = -7.2139, std = 0.2347, n = 100
  Novel sequences: mean = -7.2290, std = 0.1583, n = 100
  Difference: 0.0151
  T-test: t = 0.5303, p = 5.9648e-01
  ✗ WEAK: No significant correlation
Layer 5: r=0.038, p=5.96e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = 0.0330, p = 6.4310e-01
  Memorized sequences: mean = -7.5799, std = 0.2227, n = 100
  Novel sequences: mean = -7.5926, std = 0.1555, n = 100
  Difference: 0.0127
  T-test: t = 0.4641, p = 6.4311e-01
  ✗ WEAK: No significant correlation
Layer 6: r=0.033, p=6.43e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.0006, p = 9.9312e-01
  Memorized sequences: mean = -7.9630, std = 0.2220, n = 100
  Novel sequences: mean = -7.9627, std = 0.1575, n = 100
  Difference: -0.0002
  T-test: t = -0.0087, p = 9.9310e-01
  ✗ WEAK: No significant correlation
Layer 7: r=-0.001, p=9.93e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.0270, p = 7.0449e-01
  Memorized sequences: mean = -8.3833, std = 0.2213, n = 100
  Novel sequences: mean = -8.3729, std = 0.1605, n = 100
  Difference: -0.0104
  T-test: t = -0.3798, p = 7.0447e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.027, p=7.04e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.0235, p = 7.4140e-01
  Memorized sequences: mean = -8.8124, std = 0.2311, n = 100
  Novel sequences: mean = -8.8031, std = 0.1604, n = 100
  Difference: -0.0093
  T-test: t = -0.3304, p = 7.4143e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.023, p=7.41e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = 0.0116, p = 8.7097e-01
  Memorized sequences: mean = -7.1293, std = 0.2571, n = 100
  Novel sequences: mean = -7.1346, std = 0.1986, n = 100
  Difference: 0.0053
  T-test: t = 0.1626, p = 8.7097e-01
  ✗ WEAK: No significant correlation
Layer 10: r=0.012, p=8.71e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 10 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 10 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 10, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.0468, 'grad_norm': 11.629875183105469, 'learning_rate': 4.82e-05, 'epoch': 0.4}
{'loss': 3.8897, 'grad_norm': 14.657926559448242, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.8}
{'loss': 3.5047, 'grad_norm': 13.239726066589355, 'learning_rate': 4.4200000000000004e-05, 'epoch': 1.2}
{'loss': 3.2898, 'grad_norm': 10.760688781738281, 'learning_rate': 4.22e-05, 'epoch': 1.6}
{'loss': 3.2942, 'grad_norm': 14.743003845214844, 'learning_rate': 4.02e-05, 'epoch': 2.0}
{'loss': 2.9142, 'grad_norm': 11.767252922058105, 'learning_rate': 3.82e-05, 'epoch': 2.4}
{'loss': 2.8961, 'grad_norm': 12.405577659606934, 'learning_rate': 3.62e-05, 'epoch': 2.8}
{'loss': 2.7796, 'grad_norm': 13.402961730957031, 'learning_rate': 3.4200000000000005e-05, 'epoch': 3.2}
{'loss': 2.6205, 'grad_norm': 12.518674850463867, 'learning_rate': 3.2200000000000003e-05, 'epoch': 3.6}
{'loss': 2.5401, 'grad_norm': 13.22890567779541, 'learning_rate': 3.02e-05, 'epoch': 4.0}
{'loss': 2.3932, 'grad_norm': 11.64113712310791, 'learning_rate': 2.8199999999999998e-05, 'epoch': 4.4}
{'loss': 2.4257, 'grad_norm': 13.286124229431152, 'learning_rate': 2.6200000000000003e-05, 'epoch': 4.8}
{'loss': 2.3591, 'grad_norm': 12.702510833740234, 'learning_rate': 2.4200000000000002e-05, 'epoch': 5.2}
{'loss': 2.234, 'grad_norm': 12.712593078613281, 'learning_rate': 2.22e-05, 'epoch': 5.6}
{'loss': 2.2163, 'grad_norm': 13.594890594482422, 'learning_rate': 2.0200000000000003e-05, 'epoch': 6.0}
{'loss': 2.1314, 'grad_norm': 12.106398582458496, 'learning_rate': 1.8200000000000002e-05, 'epoch': 6.4}
{'loss': 2.1052, 'grad_norm': 12.31357192993164, 'learning_rate': 1.62e-05, 'epoch': 6.8}
{'loss': 2.0124, 'grad_norm': 13.640229225158691, 'learning_rate': 1.42e-05, 'epoch': 7.2}
{'loss': 2.0431, 'grad_norm': 11.588383674621582, 'learning_rate': 1.22e-05, 'epoch': 7.6}
{'loss': 2.0095, 'grad_norm': 13.327834129333496, 'learning_rate': 1.02e-05, 'epoch': 8.0}
{'loss': 1.9847, 'grad_norm': 13.27896785736084, 'learning_rate': 8.200000000000001e-06, 'epoch': 8.4}
{'loss': 1.9155, 'grad_norm': 15.413405418395996, 'learning_rate': 6.2e-06, 'epoch': 8.8}
{'loss': 1.9086, 'grad_norm': 11.113649368286133, 'learning_rate': 4.2000000000000004e-06, 'epoch': 9.2}
{'loss': 1.8965, 'grad_norm': 10.698290824890137, 'learning_rate': 2.2e-06, 'epoch': 9.6}
{'loss': 1.911, 'grad_norm': 11.490180015563965, 'learning_rate': 2.0000000000000002e-07, 'epoch': 10.0}
{'train_runtime': 21.7992, 'train_samples_per_second': 45.873, 'train_steps_per_second': 11.468, 'train_loss': 2.532877799987793, 'epoch': 10.0}
100% 250/250 [00:21<00:00, 11.47it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:24<00:00,  1.02s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 53.39it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.2682
  Std compression: 1.8666

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.8243
  Std compression: 1.5411

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.2898
  Std compression: 1.4880

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.6443
  Std compression: 1.4331

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.0053
  Std compression: 1.4264

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.2808
  Std compression: 1.4154

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.6364
  Std compression: 1.4120

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.0153
  Std compression: 1.4468

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4374
  Std compression: 1.4191

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.8744
  Std compression: 1.4382

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.3172
  Std compression: 1.6566

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = 0.0079, p = 9.1119e-01
  Memorized sequences: mean = -5.2214, std = 0.2289, n = 100
  Novel sequences: mean = -5.2249, std = 0.2031, n = 100
  Difference: 0.0034
  T-test: t = 0.1117, p = 9.1118e-01
  ✗ WEAK: No significant correlation
Layer 0: r=0.008, p=9.11e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = 0.0116, p = 8.7026e-01
  Memorized sequences: mean = -5.7843, std = 0.2033, n = 100
  Novel sequences: mean = -5.7886, std = 0.1728, n = 100
  Difference: 0.0044
  T-test: t = 0.1636, p = 8.7025e-01
  ✗ WEAK: No significant correlation
Layer 1: r=0.012, p=8.70e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = -0.0017, p = 9.8064e-01
  Memorized sequences: mean = -6.2523, std = 0.2119, n = 100
  Novel sequences: mean = -6.2517, std = 0.1743, n = 100
  Difference: -0.0007
  T-test: t = -0.0243, p = 9.8060e-01
  ✗ WEAK: No significant correlation
Layer 2: r=-0.002, p=9.81e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0040, p = 9.5520e-01
  Memorized sequences: mean = -6.6093, std = 0.2129, n = 100
  Novel sequences: mean = -6.6078, std = 0.1637, n = 100
  Difference: -0.0015
  T-test: t = -0.0562, p = 9.5524e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.004, p=9.55e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = 0.0284, p = 6.8953e-01
  Memorized sequences: mean = -6.9594, std = 0.2148, n = 100
  Novel sequences: mean = -6.9701, std = 0.1556, n = 100
  Difference: 0.0107
  T-test: t = 0.4000, p = 6.8958e-01
  ✗ WEAK: No significant correlation
Layer 4: r=0.028, p=6.90e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = 0.0202, p = 7.7634e-01
  Memorized sequences: mean = -7.2375, std = 0.2174, n = 100
  Novel sequences: mean = -7.2452, std = 0.1573, n = 100
  Difference: 0.0077
  T-test: t = 0.2845, p = 7.7635e-01
  ✗ WEAK: No significant correlation
Layer 5: r=0.020, p=7.76e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = 0.0197, p = 7.8218e-01
  Memorized sequences: mean = -7.5912, std = 0.2218, n = 100
  Novel sequences: mean = -7.5987, std = 0.1537, n = 100
  Difference: 0.0075
  T-test: t = 0.2768, p = 7.8223e-01
  ✗ WEAK: No significant correlation
Layer 6: r=0.020, p=7.82e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.0094, p = 8.9545e-01
  Memorized sequences: mean = -7.9792, std = 0.2270, n = 100
  Novel sequences: mean = -7.9755, std = 0.1575, n = 100
  Difference: -0.0037
  T-test: t = -0.1316, p = 8.9545e-01
  ✗ WEAK: No significant correlation
Layer 7: r=-0.009, p=8.95e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.0402, p = 5.7182e-01
  Memorized sequences: mean = -8.4086, std = 0.2230, n = 100
  Novel sequences: mean = -8.3930, std = 0.1615, n = 100
  Difference: -0.0157
  T-test: t = -0.5663, p = 5.7183e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.040, p=5.72e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.0498, p = 4.8350e-01
  Memorized sequences: mean = -8.8482, std = 0.2272, n = 100
  Novel sequences: mean = -8.8285, std = 0.1616, n = 100
  Difference: -0.0197
  T-test: t = -0.7020, p = 4.8352e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.050, p=4.83e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = -0.0451, p = 5.2603e-01
  Memorized sequences: mean = -7.3022, std = 0.2518, n = 100
  Novel sequences: mean = -7.2814, std = 0.2052, n = 100
  Difference: -0.0207
  T-test: t = -0.6352, p = 5.2600e-01
  ✗ WEAK: No significant correlation
Layer 10: r=-0.045, p=5.26e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 20 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 20 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 20, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.0463, 'grad_norm': 11.627224922180176, 'learning_rate': 4.91e-05, 'epoch': 0.4}
{'loss': 3.8871, 'grad_norm': 14.683355331420898, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.8}
{'loss': 3.4963, 'grad_norm': 13.214375495910645, 'learning_rate': 4.71e-05, 'epoch': 1.2}
{'loss': 3.272, 'grad_norm': 10.814498901367188, 'learning_rate': 4.61e-05, 'epoch': 1.6}
{'loss': 3.2725, 'grad_norm': 14.79586410522461, 'learning_rate': 4.5100000000000005e-05, 'epoch': 2.0}
{'loss': 2.8678, 'grad_norm': 11.979693412780762, 'learning_rate': 4.41e-05, 'epoch': 2.4}
{'loss': 2.8497, 'grad_norm': 12.540629386901855, 'learning_rate': 4.3100000000000004e-05, 'epoch': 2.8}
{'loss': 2.7176, 'grad_norm': 13.520061492919922, 'learning_rate': 4.21e-05, 'epoch': 3.2}
{'loss': 2.5444, 'grad_norm': 12.684358596801758, 'learning_rate': 4.11e-05, 'epoch': 3.6}
{'loss': 2.4627, 'grad_norm': 13.419174194335938, 'learning_rate': 4.0100000000000006e-05, 'epoch': 4.0}
{'loss': 2.2781, 'grad_norm': 11.892134666442871, 'learning_rate': 3.91e-05, 'epoch': 4.4}
{'loss': 2.3062, 'grad_norm': 13.068744659423828, 'learning_rate': 3.8100000000000005e-05, 'epoch': 4.8}
{'loss': 2.22, 'grad_norm': 12.714376449584961, 'learning_rate': 3.71e-05, 'epoch': 5.2}
{'loss': 2.0651, 'grad_norm': 12.698741912841797, 'learning_rate': 3.61e-05, 'epoch': 5.6}
{'loss': 2.0502, 'grad_norm': 13.518631935119629, 'learning_rate': 3.51e-05, 'epoch': 6.0}
{'loss': 1.8972, 'grad_norm': 11.699085235595703, 'learning_rate': 3.41e-05, 'epoch': 6.4}
{'loss': 1.8729, 'grad_norm': 12.058185577392578, 'learning_rate': 3.3100000000000005e-05, 'epoch': 6.8}
{'loss': 1.7472, 'grad_norm': 12.57247543334961, 'learning_rate': 3.21e-05, 'epoch': 7.2}
{'loss': 1.726, 'grad_norm': 10.90651798248291, 'learning_rate': 3.1100000000000004e-05, 'epoch': 7.6}
{'loss': 1.7105, 'grad_norm': 12.660305976867676, 'learning_rate': 3.01e-05, 'epoch': 8.0}
{'loss': 1.5893, 'grad_norm': 12.150019645690918, 'learning_rate': 2.91e-05, 'epoch': 8.4}
{'loss': 1.5201, 'grad_norm': 14.442398071289062, 'learning_rate': 2.8100000000000005e-05, 'epoch': 8.8}
{'loss': 1.4572, 'grad_norm': 9.931902885437012, 'learning_rate': 2.7100000000000005e-05, 'epoch': 9.2}
{'loss': 1.3898, 'grad_norm': 9.092399597167969, 'learning_rate': 2.61e-05, 'epoch': 9.6}
{'loss': 1.4158, 'grad_norm': 10.18551254272461, 'learning_rate': 2.51e-05, 'epoch': 10.0}
{'loss': 1.2993, 'grad_norm': 10.38432788848877, 'learning_rate': 2.41e-05, 'epoch': 10.4}
{'loss': 1.264, 'grad_norm': 10.918081283569336, 'learning_rate': 2.3100000000000002e-05, 'epoch': 10.8}
{'loss': 1.2641, 'grad_norm': 11.21112060546875, 'learning_rate': 2.2100000000000002e-05, 'epoch': 11.2}
{'loss': 1.1008, 'grad_norm': 9.373740196228027, 'learning_rate': 2.11e-05, 'epoch': 11.6}
{'loss': 1.1688, 'grad_norm': 10.26177978515625, 'learning_rate': 2.01e-05, 'epoch': 12.0}
{'loss': 1.049, 'grad_norm': 8.654491424560547, 'learning_rate': 1.91e-05, 'epoch': 12.4}
{'loss': 1.0476, 'grad_norm': 8.363565444946289, 'learning_rate': 1.81e-05, 'epoch': 12.8}
{'loss': 1.0165, 'grad_norm': 9.073897361755371, 'learning_rate': 1.7100000000000002e-05, 'epoch': 13.2}
{'loss': 0.9745, 'grad_norm': 8.99764347076416, 'learning_rate': 1.6100000000000002e-05, 'epoch': 13.6}
{'loss': 0.9857, 'grad_norm': 8.254118919372559, 'learning_rate': 1.51e-05, 'epoch': 14.0}
{'loss': 0.9509, 'grad_norm': 6.761265277862549, 'learning_rate': 1.4099999999999999e-05, 'epoch': 14.4}
{'loss': 0.8387, 'grad_norm': 7.47541618347168, 'learning_rate': 1.3100000000000002e-05, 'epoch': 14.8}
{'loss': 0.9012, 'grad_norm': 8.105323791503906, 'learning_rate': 1.2100000000000001e-05, 'epoch': 15.2}
{'loss': 0.8779, 'grad_norm': 8.200651168823242, 'learning_rate': 1.11e-05, 'epoch': 15.6}
{'loss': 0.8304, 'grad_norm': 8.758949279785156, 'learning_rate': 1.0100000000000002e-05, 'epoch': 16.0}
{'loss': 0.8019, 'grad_norm': 6.455954551696777, 'learning_rate': 9.100000000000001e-06, 'epoch': 16.4}
{'loss': 0.8333, 'grad_norm': 7.365049839019775, 'learning_rate': 8.1e-06, 'epoch': 16.8}
{'loss': 0.7791, 'grad_norm': 9.028864860534668, 'learning_rate': 7.1e-06, 'epoch': 17.2}
{'loss': 0.7394, 'grad_norm': 7.302044868469238, 'learning_rate': 6.1e-06, 'epoch': 17.6}
{'loss': 0.7818, 'grad_norm': 8.518799781799316, 'learning_rate': 5.1e-06, 'epoch': 18.0}
{'loss': 0.7199, 'grad_norm': 7.335132598876953, 'learning_rate': 4.1000000000000006e-06, 'epoch': 18.4}
{'loss': 0.7724, 'grad_norm': 7.854123592376709, 'learning_rate': 3.1e-06, 'epoch': 18.8}
{'loss': 0.7475, 'grad_norm': 7.037497520446777, 'learning_rate': 2.1000000000000002e-06, 'epoch': 19.2}
{'loss': 0.7422, 'grad_norm': 6.900516510009766, 'learning_rate': 1.1e-06, 'epoch': 19.6}
{'loss': 0.7352, 'grad_norm': 6.732315540313721, 'learning_rate': 1.0000000000000001e-07, 'epoch': 20.0}
{'train_runtime': 43.3003, 'train_samples_per_second': 46.189, 'train_steps_per_second': 11.547, 'train_loss': 1.6376812000274659, 'epoch': 20.0}
100% 500/500 [00:43<00:00, 11.55it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:26<00:00,  1.03s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 52.77it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.3147
  Std compression: 1.8388

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.8596
  Std compression: 1.5341

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.3238
  Std compression: 1.4497

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.6843
  Std compression: 1.4178

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.0408
  Std compression: 1.4201

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.3193
  Std compression: 1.3896

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.6650
  Std compression: 1.3878

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.0426
  Std compression: 1.3945

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4603
  Std compression: 1.4073

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.9146
  Std compression: 1.4226

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.9411
  Std compression: 1.5818

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = -0.0056, p = 9.3739e-01
  Memorized sequences: mean = -5.2740, std = 0.2262, n = 100
  Novel sequences: mean = -5.2716, std = 0.2026, n = 100
  Difference: -0.0024
  T-test: t = -0.0786, p = 9.3740e-01
  ✗ WEAK: No significant correlation
Layer 0: r=-0.006, p=9.37e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = -0.0085, p = 9.0440e-01
  Memorized sequences: mean = -5.8258, std = 0.1995, n = 100
  Novel sequences: mean = -5.8226, std = 0.1706, n = 100
  Difference: -0.0032
  T-test: t = -0.1203, p = 9.0437e-01
  ✗ WEAK: No significant correlation
Layer 1: r=-0.009, p=9.04e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = -0.0268, p = 7.0677e-01
  Memorized sequences: mean = -6.2963, std = 0.2016, n = 100
  Novel sequences: mean = -6.2864, std = 0.1672, n = 100
  Difference: -0.0099
  T-test: t = -0.3767, p = 7.0678e-01
  ✗ WEAK: No significant correlation
Layer 2: r=-0.027, p=7.07e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0225, p = 7.5196e-01
  Memorized sequences: mean = -6.6558, std = 0.2079, n = 100
  Novel sequences: mean = -6.6474, std = 0.1628, n = 100
  Difference: -0.0084
  T-test: t = -0.3166, p = 7.5190e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.022, p=7.52e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = 0.0051, p = 9.4255e-01
  Memorized sequences: mean = -7.0029, std = 0.2080, n = 100
  Novel sequences: mean = -7.0048, std = 0.1528, n = 100
  Difference: 0.0019
  T-test: t = 0.0722, p = 9.4251e-01
  ✗ WEAK: No significant correlation
Layer 4: r=0.005, p=9.43e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = -0.0084, p = 9.0549e-01
  Memorized sequences: mean = -7.2860, std = 0.2116, n = 100
  Novel sequences: mean = -7.2828, std = 0.1533, n = 100
  Difference: -0.0031
  T-test: t = -0.1189, p = 9.0546e-01
  ✗ WEAK: No significant correlation
Layer 5: r=-0.008, p=9.05e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = -0.0143, p = 8.4086e-01
  Memorized sequences: mean = -7.6312, std = 0.2157, n = 100
  Novel sequences: mean = -7.6259, std = 0.1496, n = 100
  Difference: -0.0053
  T-test: t = -0.2010, p = 8.4087e-01
  ✗ WEAK: No significant correlation
Layer 6: r=-0.014, p=8.41e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.0549, p = 4.4046e-01
  Memorized sequences: mean = -8.0200, std = 0.2135, n = 100
  Novel sequences: mean = -7.9996, std = 0.1526, n = 100
  Difference: -0.0204
  T-test: t = -0.7730, p = 4.4046e-01
  ✗ WEAK: No significant correlation
Layer 7: r=-0.055, p=4.40e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.0686, p = 3.3470e-01
  Memorized sequences: mean = -8.4393, std = 0.2217, n = 100
  Novel sequences: mean = -8.4130, std = 0.1557, n = 100
  Difference: -0.0263
  T-test: t = -0.9671, p = 3.3470e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.069, p=3.35e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.0878, p = 2.1659e-01
  Memorized sequences: mean = -8.8978, std = 0.2226, n = 100
  Novel sequences: mean = -8.8641, std = 0.1541, n = 100
  Difference: -0.0337
  T-test: t = -1.2396, p = 2.1659e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.088, p=2.17e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = -0.0768, p = 2.8003e-01
  Memorized sequences: mean = -7.9360, std = 0.2276, n = 100
  Novel sequences: mean = -7.9039, std = 0.1872, n = 100
  Difference: -0.0321
  T-test: t = -1.0832, p = 2.8003e-01
  ✗ WEAK: No significant correlation
Layer 10: r=-0.077, p=2.80e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 30 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 30 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 30, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.0462, 'grad_norm': 11.626333236694336, 'learning_rate': 4.94e-05, 'epoch': 0.4}
{'loss': 3.8862, 'grad_norm': 14.691253662109375, 'learning_rate': 4.8733333333333337e-05, 'epoch': 0.8}
{'loss': 3.4936, 'grad_norm': 13.206440925598145, 'learning_rate': 4.806666666666667e-05, 'epoch': 1.2}
{'loss': 3.2662, 'grad_norm': 10.83265495300293, 'learning_rate': 4.74e-05, 'epoch': 1.6}
{'loss': 3.2655, 'grad_norm': 14.831182479858398, 'learning_rate': 4.6733333333333335e-05, 'epoch': 2.0}
{'loss': 2.8528, 'grad_norm': 12.048929214477539, 'learning_rate': 4.606666666666667e-05, 'epoch': 2.4}
{'loss': 2.835, 'grad_norm': 12.60135269165039, 'learning_rate': 4.5400000000000006e-05, 'epoch': 2.8}
{'loss': 2.6981, 'grad_norm': 13.543606758117676, 'learning_rate': 4.473333333333334e-05, 'epoch': 3.2}
{'loss': 2.5206, 'grad_norm': 12.743145942687988, 'learning_rate': 4.406666666666667e-05, 'epoch': 3.6}
{'loss': 2.4389, 'grad_norm': 13.4841890335083, 'learning_rate': 4.3400000000000005e-05, 'epoch': 4.0}
{'loss': 2.2422, 'grad_norm': 11.934061050415039, 'learning_rate': 4.273333333333333e-05, 'epoch': 4.4}
{'loss': 2.2691, 'grad_norm': 13.011114120483398, 'learning_rate': 4.206666666666667e-05, 'epoch': 4.8}
{'loss': 2.1773, 'grad_norm': 12.636667251586914, 'learning_rate': 4.14e-05, 'epoch': 5.2}
{'loss': 2.0133, 'grad_norm': 12.681610107421875, 'learning_rate': 4.073333333333333e-05, 'epoch': 5.6}
{'loss': 2.0007, 'grad_norm': 13.462944030761719, 'learning_rate': 4.006666666666667e-05, 'epoch': 6.0}
{'loss': 1.8264, 'grad_norm': 11.448741912841797, 'learning_rate': 3.94e-05, 'epoch': 6.4}
{'loss': 1.803, 'grad_norm': 11.9651517868042, 'learning_rate': 3.873333333333333e-05, 'epoch': 6.8}
{'loss': 1.6682, 'grad_norm': 12.293856620788574, 'learning_rate': 3.8066666666666666e-05, 'epoch': 7.2}
{'loss': 1.6305, 'grad_norm': 10.853890419006348, 'learning_rate': 3.74e-05, 'epoch': 7.6}
{'loss': 1.6224, 'grad_norm': 12.38535213470459, 'learning_rate': 3.6733333333333336e-05, 'epoch': 8.0}
{'loss': 1.4749, 'grad_norm': 11.745538711547852, 'learning_rate': 3.606666666666667e-05, 'epoch': 8.4}
{'loss': 1.4074, 'grad_norm': 14.270708084106445, 'learning_rate': 3.54e-05, 'epoch': 8.8}
{'loss': 1.3307, 'grad_norm': 9.351696968078613, 'learning_rate': 3.4733333333333335e-05, 'epoch': 9.2}
{'loss': 1.252, 'grad_norm': 8.453853607177734, 'learning_rate': 3.406666666666667e-05, 'epoch': 9.6}
{'loss': 1.2775, 'grad_norm': 9.788522720336914, 'learning_rate': 3.3400000000000005e-05, 'epoch': 10.0}
{'loss': 1.1274, 'grad_norm': 9.56705379486084, 'learning_rate': 3.2733333333333334e-05, 'epoch': 10.4}
{'loss': 1.0996, 'grad_norm': 10.383095741271973, 'learning_rate': 3.206666666666667e-05, 'epoch': 10.8}
{'loss': 1.0852, 'grad_norm': 10.060195922851562, 'learning_rate': 3.1400000000000004e-05, 'epoch': 11.2}
{'loss': 0.9159, 'grad_norm': 8.446202278137207, 'learning_rate': 3.073333333333334e-05, 'epoch': 11.6}
{'loss': 0.9704, 'grad_norm': 9.304113388061523, 'learning_rate': 3.006666666666667e-05, 'epoch': 12.0}
{'loss': 0.8339, 'grad_norm': 7.622525691986084, 'learning_rate': 2.94e-05, 'epoch': 12.4}
{'loss': 0.8261, 'grad_norm': 7.36427640914917, 'learning_rate': 2.8733333333333335e-05, 'epoch': 12.8}
{'loss': 0.7884, 'grad_norm': 7.956658840179443, 'learning_rate': 2.806666666666667e-05, 'epoch': 13.2}
{'loss': 0.7286, 'grad_norm': 7.676166534423828, 'learning_rate': 2.7400000000000002e-05, 'epoch': 13.6}
{'loss': 0.744, 'grad_norm': 7.268264293670654, 'learning_rate': 2.6733333333333334e-05, 'epoch': 14.0}
{'loss': 0.6724, 'grad_norm': 5.4107561111450195, 'learning_rate': 2.6066666666666666e-05, 'epoch': 14.4}
{'loss': 0.5995, 'grad_norm': 6.1485443115234375, 'learning_rate': 2.54e-05, 'epoch': 14.8}
{'loss': 0.6251, 'grad_norm': 6.058419227600098, 'learning_rate': 2.4733333333333333e-05, 'epoch': 15.2}
{'loss': 0.5849, 'grad_norm': 6.765392780303955, 'learning_rate': 2.4066666666666668e-05, 'epoch': 15.6}
{'loss': 0.5596, 'grad_norm': 6.79257345199585, 'learning_rate': 2.3400000000000003e-05, 'epoch': 16.0}
{'loss': 0.4952, 'grad_norm': 5.435358047485352, 'learning_rate': 2.2733333333333335e-05, 'epoch': 16.4}
{'loss': 0.5187, 'grad_norm': 5.635787010192871, 'learning_rate': 2.206666666666667e-05, 'epoch': 16.8}
{'loss': 0.4819, 'grad_norm': 6.898002624511719, 'learning_rate': 2.1400000000000002e-05, 'epoch': 17.2}
{'loss': 0.4323, 'grad_norm': 5.304788589477539, 'learning_rate': 2.0733333333333334e-05, 'epoch': 17.6}
{'loss': 0.4499, 'grad_norm': 6.060234546661377, 'learning_rate': 2.0066666666666665e-05, 'epoch': 18.0}
{'loss': 0.3828, 'grad_norm': 4.653262615203857, 'learning_rate': 1.94e-05, 'epoch': 18.4}
{'loss': 0.4157, 'grad_norm': 5.672366142272949, 'learning_rate': 1.8733333333333332e-05, 'epoch': 18.8}
{'loss': 0.3923, 'grad_norm': 4.447774887084961, 'learning_rate': 1.8066666666666668e-05, 'epoch': 19.2}
{'loss': 0.3633, 'grad_norm': 4.506901741027832, 'learning_rate': 1.74e-05, 'epoch': 19.6}
{'loss': 0.3661, 'grad_norm': 4.77205228805542, 'learning_rate': 1.6733333333333335e-05, 'epoch': 20.0}
{'loss': 0.3287, 'grad_norm': 4.926877021789551, 'learning_rate': 1.606666666666667e-05, 'epoch': 20.4}
{'loss': 0.3439, 'grad_norm': 5.443778038024902, 'learning_rate': 1.54e-05, 'epoch': 20.8}
{'loss': 0.335, 'grad_norm': 4.978250503540039, 'learning_rate': 1.4733333333333335e-05, 'epoch': 21.2}
{'loss': 0.3102, 'grad_norm': 3.479081630706787, 'learning_rate': 1.4066666666666667e-05, 'epoch': 21.6}
{'loss': 0.3003, 'grad_norm': 4.122827053070068, 'learning_rate': 1.3400000000000002e-05, 'epoch': 22.0}
{'loss': 0.2772, 'grad_norm': 4.256211757659912, 'learning_rate': 1.2733333333333334e-05, 'epoch': 22.4}
{'loss': 0.2791, 'grad_norm': 4.482414245605469, 'learning_rate': 1.2066666666666667e-05, 'epoch': 22.8}
{'loss': 0.2666, 'grad_norm': 4.517167091369629, 'learning_rate': 1.1400000000000001e-05, 'epoch': 23.2}
{'loss': 0.2898, 'grad_norm': 4.723968982696533, 'learning_rate': 1.0733333333333334e-05, 'epoch': 23.6}
{'loss': 0.2692, 'grad_norm': 4.071115970611572, 'learning_rate': 1.0066666666666668e-05, 'epoch': 24.0}
{'loss': 0.2552, 'grad_norm': 3.146111011505127, 'learning_rate': 9.4e-06, 'epoch': 24.4}
{'loss': 0.2434, 'grad_norm': 4.08113431930542, 'learning_rate': 8.733333333333333e-06, 'epoch': 24.8}
{'loss': 0.2281, 'grad_norm': 3.977250576019287, 'learning_rate': 8.066666666666667e-06, 'epoch': 25.2}
{'loss': 0.2422, 'grad_norm': 3.6465322971343994, 'learning_rate': 7.4e-06, 'epoch': 25.6}
{'loss': 0.2501, 'grad_norm': 3.7966995239257812, 'learning_rate': 6.733333333333333e-06, 'epoch': 26.0}
{'loss': 0.2305, 'grad_norm': 3.3493993282318115, 'learning_rate': 6.066666666666667e-06, 'epoch': 26.4}
{'loss': 0.2369, 'grad_norm': 3.819420576095581, 'learning_rate': 5.4e-06, 'epoch': 26.8}
{'loss': 0.2346, 'grad_norm': 3.60593581199646, 'learning_rate': 4.7333333333333335e-06, 'epoch': 27.2}
{'loss': 0.2194, 'grad_norm': 4.078189373016357, 'learning_rate': 4.066666666666666e-06, 'epoch': 27.6}
{'loss': 0.228, 'grad_norm': 4.990918159484863, 'learning_rate': 3.4000000000000005e-06, 'epoch': 28.0}
{'loss': 0.2153, 'grad_norm': 3.3558480739593506, 'learning_rate': 2.7333333333333336e-06, 'epoch': 28.4}
{'loss': 0.2103, 'grad_norm': 3.3440215587615967, 'learning_rate': 2.0666666666666666e-06, 'epoch': 28.8}
{'loss': 0.219, 'grad_norm': 4.277369976043701, 'learning_rate': 1.4000000000000001e-06, 'epoch': 29.2}
{'loss': 0.2145, 'grad_norm': 3.826164960861206, 'learning_rate': 7.333333333333333e-07, 'epoch': 29.6}
{'loss': 0.2024, 'grad_norm': 3.363976240158081, 'learning_rate': 6.666666666666667e-08, 'epoch': 30.0}
{'train_runtime': 64.5496, 'train_samples_per_second': 46.476, 'train_steps_per_second': 11.619, 'train_loss': 1.0691677338282268, 'epoch': 30.0}
100% 750/750 [01:04<00:00, 11.62it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:22<00:00,  1.01s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 53.31it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.3683
  Std compression: 1.8431

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.9122
  Std compression: 1.5866

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.3698
  Std compression: 1.4741

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.7398
  Std compression: 1.4377

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.0989
  Std compression: 1.4263

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.3738
  Std compression: 1.4036

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.7130
  Std compression: 1.4248

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.0836
  Std compression: 1.4313

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4896
  Std compression: 1.4156

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.9371
  Std compression: 1.4741

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.3083
  Std compression: 1.6023

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = -0.0113, p = 8.7356e-01
  Memorized sequences: mean = -5.3308, std = 0.2290, n = 100
  Novel sequences: mean = -5.3259, std = 0.2002, n = 100
  Difference: -0.0049
  T-test: t = -0.1594, p = 8.7354e-01
  ✗ WEAK: No significant correlation
Layer 0: r=-0.011, p=8.74e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = -0.0070, p = 9.2195e-01
  Memorized sequences: mean = -5.8791, std = 0.2080, n = 100
  Novel sequences: mean = -5.8764, std = 0.1724, n = 100
  Difference: -0.0027
  T-test: t = -0.0981, p = 9.2194e-01
  ✗ WEAK: No significant correlation
Layer 1: r=-0.007, p=9.22e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = -0.0386, p = 5.8767e-01
  Memorized sequences: mean = -6.3466, std = 0.2035, n = 100
  Novel sequences: mean = -6.3321, std = 0.1705, n = 100
  Difference: -0.0145
  T-test: t = -0.5431, p = 5.8768e-01
  ✗ WEAK: No significant correlation
Layer 2: r=-0.039, p=5.88e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0397, p = 5.7688e-01
  Memorized sequences: mean = -6.7179, std = 0.2084, n = 100
  Novel sequences: mean = -6.7030, std = 0.1620, n = 100
  Difference: -0.0148
  T-test: t = -0.5589, p = 5.7686e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.040, p=5.77e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = -0.0102, p = 8.8552e-01
  Memorized sequences: mean = -7.0663, std = 0.2131, n = 100
  Novel sequences: mean = -7.0625, std = 0.1523, n = 100
  Difference: -0.0038
  T-test: t = -0.1442, p = 8.8550e-01
  ✗ WEAK: No significant correlation
Layer 4: r=-0.010, p=8.86e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = -0.0276, p = 6.9849e-01
  Memorized sequences: mean = -7.3468, std = 0.2129, n = 100
  Novel sequences: mean = -7.3366, std = 0.1529, n = 100
  Difference: -0.0102
  T-test: t = -0.3879, p = 6.9850e-01
  ✗ WEAK: No significant correlation
Layer 5: r=-0.028, p=6.98e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = -0.0348, p = 6.2507e-01
  Memorized sequences: mean = -7.6861, std = 0.2174, n = 100
  Novel sequences: mean = -7.6730, std = 0.1523, n = 100
  Difference: -0.0131
  T-test: t = -0.4894, p = 6.2508e-01
  ✗ WEAK: No significant correlation
Layer 6: r=-0.035, p=6.25e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.0733, p = 3.0260e-01
  Memorized sequences: mean = -8.0666, std = 0.2205, n = 100
  Novel sequences: mean = -8.0387, std = 0.1520, n = 100
  Difference: -0.0278
  T-test: t = -1.0336, p = 3.0258e-01
  ✗ WEAK: No significant correlation
Layer 7: r=-0.073, p=3.03e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.0937, p = 1.8708e-01
  Memorized sequences: mean = -8.4751, std = 0.2212, n = 100
  Novel sequences: mean = -8.4390, std = 0.1574, n = 100
  Difference: -0.0361
  T-test: t = -1.3239, p = 1.8708e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.094, p=1.87e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.1002, p = 1.5793e-01
  Memorized sequences: mean = -8.9233, std = 0.2318, n = 100
  Novel sequences: mean = -8.8837, std = 0.1541, n = 100
  Difference: -0.0396
  T-test: t = -1.4174, p = 1.5793e-01
  ✗ WEAK: No significant correlation
Layer 9: r=-0.100, p=1.58e-01

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = -0.0830, p = 2.4254e-01
  Memorized sequences: mean = -8.3087, std = 0.2151, n = 100
  Novel sequences: mean = -8.2758, std = 0.1781, n = 100
  Difference: -0.0329
  T-test: t = -1.1721, p = 2.4256e-01
  ✗ WEAK: No significant correlation
Layer 10: r=-0.083, p=2.43e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 50 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 50 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 50, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.046, 'grad_norm': 11.625606536865234, 'learning_rate': 4.9640000000000006e-05, 'epoch': 0.4}
{'loss': 3.8856, 'grad_norm': 14.697293281555176, 'learning_rate': 4.924e-05, 'epoch': 0.8}
{'loss': 3.4914, 'grad_norm': 13.20022201538086, 'learning_rate': 4.884e-05, 'epoch': 1.2}
{'loss': 3.2615, 'grad_norm': 10.847270011901855, 'learning_rate': 4.8440000000000004e-05, 'epoch': 1.6}
{'loss': 3.26, 'grad_norm': 14.86411190032959, 'learning_rate': 4.804e-05, 'epoch': 2.0}
{'loss': 2.8409, 'grad_norm': 12.102326393127441, 'learning_rate': 4.7640000000000005e-05, 'epoch': 2.4}
{'loss': 2.8236, 'grad_norm': 12.663021087646484, 'learning_rate': 4.724e-05, 'epoch': 2.8}
{'loss': 2.6828, 'grad_norm': 13.555975914001465, 'learning_rate': 4.684e-05, 'epoch': 3.2}
{'loss': 2.5023, 'grad_norm': 12.797628402709961, 'learning_rate': 4.644e-05, 'epoch': 3.6}
{'loss': 2.4204, 'grad_norm': 13.534480094909668, 'learning_rate': 4.604e-05, 'epoch': 4.0}
{'loss': 2.2143, 'grad_norm': 11.971677780151367, 'learning_rate': 4.564e-05, 'epoch': 4.4}
{'loss': 2.2404, 'grad_norm': 12.980680465698242, 'learning_rate': 4.524000000000001e-05, 'epoch': 4.8}
{'loss': 2.1445, 'grad_norm': 12.558292388916016, 'learning_rate': 4.4840000000000004e-05, 'epoch': 5.2}
{'loss': 1.9731, 'grad_norm': 12.641327857971191, 'learning_rate': 4.444e-05, 'epoch': 5.6}
{'loss': 1.9625, 'grad_norm': 13.428399085998535, 'learning_rate': 4.4040000000000005e-05, 'epoch': 6.0}
{'loss': 1.7714, 'grad_norm': 11.192301750183105, 'learning_rate': 4.364e-05, 'epoch': 6.4}
{'loss': 1.7493, 'grad_norm': 11.869781494140625, 'learning_rate': 4.324e-05, 'epoch': 6.8}
{'loss': 1.6082, 'grad_norm': 12.139833450317383, 'learning_rate': 4.284e-05, 'epoch': 7.2}
{'loss': 1.558, 'grad_norm': 10.778162956237793, 'learning_rate': 4.244e-05, 'epoch': 7.6}
{'loss': 1.556, 'grad_norm': 12.13886833190918, 'learning_rate': 4.2040000000000004e-05, 'epoch': 8.0}
{'loss': 1.3894, 'grad_norm': 11.441115379333496, 'learning_rate': 4.164e-05, 'epoch': 8.4}
{'loss': 1.3205, 'grad_norm': 13.902853012084961, 'learning_rate': 4.124e-05, 'epoch': 8.8}
{'loss': 1.2347, 'grad_norm': 8.882877349853516, 'learning_rate': 4.084e-05, 'epoch': 9.2}
{'loss': 1.1475, 'grad_norm': 8.048666954040527, 'learning_rate': 4.044e-05, 'epoch': 9.6}
{'loss': 1.1756, 'grad_norm': 9.443343162536621, 'learning_rate': 4.004e-05, 'epoch': 10.0}
{'loss': 1.0041, 'grad_norm': 8.965299606323242, 'learning_rate': 3.964e-05, 'epoch': 10.4}
{'loss': 0.984, 'grad_norm': 9.931818008422852, 'learning_rate': 3.9240000000000004e-05, 'epoch': 10.8}
{'loss': 0.9643, 'grad_norm': 9.239631652832031, 'learning_rate': 3.884e-05, 'epoch': 11.2}
{'loss': 0.7919, 'grad_norm': 7.692746639251709, 'learning_rate': 3.8440000000000005e-05, 'epoch': 11.6}
{'loss': 0.8472, 'grad_norm': 8.563833236694336, 'learning_rate': 3.804e-05, 'epoch': 12.0}
{'loss': 0.6966, 'grad_norm': 6.822774887084961, 'learning_rate': 3.7640000000000006e-05, 'epoch': 12.4}
{'loss': 0.6873, 'grad_norm': 6.772045612335205, 'learning_rate': 3.724e-05, 'epoch': 12.8}
{'loss': 0.6481, 'grad_norm': 7.410964488983154, 'learning_rate': 3.684e-05, 'epoch': 13.2}
{'loss': 0.5852, 'grad_norm': 6.980690956115723, 'learning_rate': 3.6440000000000003e-05, 'epoch': 13.6}
{'loss': 0.6006, 'grad_norm': 6.589362621307373, 'learning_rate': 3.604e-05, 'epoch': 14.0}
{'loss': 0.5204, 'grad_norm': 4.739497661590576, 'learning_rate': 3.5640000000000004e-05, 'epoch': 14.4}
{'loss': 0.4725, 'grad_norm': 5.4376726150512695, 'learning_rate': 3.524e-05, 'epoch': 14.8}
{'loss': 0.4785, 'grad_norm': 4.994305610656738, 'learning_rate': 3.484e-05, 'epoch': 15.2}
{'loss': 0.4344, 'grad_norm': 5.729361057281494, 'learning_rate': 3.444e-05, 'epoch': 15.6}
{'loss': 0.4228, 'grad_norm': 5.574902534484863, 'learning_rate': 3.404e-05, 'epoch': 16.0}
{'loss': 0.3517, 'grad_norm': 4.833689212799072, 'learning_rate': 3.3639999999999996e-05, 'epoch': 16.4}
{'loss': 0.375, 'grad_norm': 4.614025592803955, 'learning_rate': 3.324e-05, 'epoch': 16.8}
{'loss': 0.354, 'grad_norm': 5.618873119354248, 'learning_rate': 3.2840000000000004e-05, 'epoch': 17.2}
{'loss': 0.3027, 'grad_norm': 5.887198448181152, 'learning_rate': 3.244e-05, 'epoch': 17.6}
{'loss': 0.3161, 'grad_norm': 4.585218906402588, 'learning_rate': 3.2040000000000005e-05, 'epoch': 18.0}
{'loss': 0.2588, 'grad_norm': 3.799504280090332, 'learning_rate': 3.164e-05, 'epoch': 18.4}
{'loss': 0.2833, 'grad_norm': 4.716686725616455, 'learning_rate': 3.1240000000000006e-05, 'epoch': 18.8}
{'loss': 0.2643, 'grad_norm': 3.2143943309783936, 'learning_rate': 3.084e-05, 'epoch': 19.2}
{'loss': 0.232, 'grad_norm': 3.4211766719818115, 'learning_rate': 3.0440000000000003e-05, 'epoch': 19.6}
{'loss': 0.2418, 'grad_norm': 3.9548442363739014, 'learning_rate': 3.004e-05, 'epoch': 20.0}
{'loss': 0.2074, 'grad_norm': 4.0366106033325195, 'learning_rate': 2.964e-05, 'epoch': 20.4}
{'loss': 0.2297, 'grad_norm': 3.9261975288391113, 'learning_rate': 2.924e-05, 'epoch': 20.8}
{'loss': 0.2072, 'grad_norm': 3.5759174823760986, 'learning_rate': 2.8840000000000002e-05, 'epoch': 21.2}
{'loss': 0.196, 'grad_norm': 2.500312328338623, 'learning_rate': 2.844e-05, 'epoch': 21.6}
{'loss': 0.1926, 'grad_norm': 3.235494613647461, 'learning_rate': 2.804e-05, 'epoch': 22.0}
{'loss': 0.1669, 'grad_norm': 3.1507556438446045, 'learning_rate': 2.764e-05, 'epoch': 22.4}
{'loss': 0.1697, 'grad_norm': 2.412891387939453, 'learning_rate': 2.724e-05, 'epoch': 22.8}
{'loss': 0.1662, 'grad_norm': 3.558300733566284, 'learning_rate': 2.6840000000000004e-05, 'epoch': 23.2}
{'loss': 0.177, 'grad_norm': 3.2156200408935547, 'learning_rate': 2.6440000000000004e-05, 'epoch': 23.6}
{'loss': 0.1629, 'grad_norm': 2.412977457046509, 'learning_rate': 2.6040000000000005e-05, 'epoch': 24.0}
{'loss': 0.1465, 'grad_norm': 2.1479272842407227, 'learning_rate': 2.5640000000000002e-05, 'epoch': 24.4}
{'loss': 0.1418, 'grad_norm': 2.9222028255462646, 'learning_rate': 2.5240000000000002e-05, 'epoch': 24.8}
{'loss': 0.1343, 'grad_norm': 2.405568838119507, 'learning_rate': 2.4840000000000003e-05, 'epoch': 25.2}
{'loss': 0.138, 'grad_norm': 2.5937154293060303, 'learning_rate': 2.4440000000000003e-05, 'epoch': 25.6}
{'loss': 0.1436, 'grad_norm': 2.718949794769287, 'learning_rate': 2.404e-05, 'epoch': 26.0}
{'loss': 0.1245, 'grad_norm': 2.1383798122406006, 'learning_rate': 2.364e-05, 'epoch': 26.4}
{'loss': 0.1251, 'grad_norm': 2.633970022201538, 'learning_rate': 2.324e-05, 'epoch': 26.8}
{'loss': 0.1297, 'grad_norm': 2.151709794998169, 'learning_rate': 2.284e-05, 'epoch': 27.2}
{'loss': 0.1167, 'grad_norm': 2.8511993885040283, 'learning_rate': 2.244e-05, 'epoch': 27.6}
{'loss': 0.1258, 'grad_norm': 3.5031707286834717, 'learning_rate': 2.2040000000000002e-05, 'epoch': 28.0}
{'loss': 0.1131, 'grad_norm': 2.470855712890625, 'learning_rate': 2.1640000000000003e-05, 'epoch': 28.4}
{'loss': 0.1063, 'grad_norm': 2.578089952468872, 'learning_rate': 2.124e-05, 'epoch': 28.8}
{'loss': 0.1092, 'grad_norm': 2.663501262664795, 'learning_rate': 2.084e-05, 'epoch': 29.2}
{'loss': 0.103, 'grad_norm': 2.2144956588745117, 'learning_rate': 2.044e-05, 'epoch': 29.6}
{'loss': 0.1027, 'grad_norm': 2.1132962703704834, 'learning_rate': 2.004e-05, 'epoch': 30.0}
{'loss': 0.1013, 'grad_norm': 2.3722267150878906, 'learning_rate': 1.9640000000000002e-05, 'epoch': 30.4}
{'loss': 0.0972, 'grad_norm': 1.43496835231781, 'learning_rate': 1.924e-05, 'epoch': 30.8}
{'loss': 0.0977, 'grad_norm': 1.5542171001434326, 'learning_rate': 1.8840000000000003e-05, 'epoch': 31.2}
{'loss': 0.0818, 'grad_norm': 1.5257993936538696, 'learning_rate': 1.8440000000000003e-05, 'epoch': 31.6}
{'loss': 0.1039, 'grad_norm': 2.436816930770874, 'learning_rate': 1.804e-05, 'epoch': 32.0}
{'loss': 0.0858, 'grad_norm': 1.6388505697250366, 'learning_rate': 1.764e-05, 'epoch': 32.4}
{'loss': 0.0917, 'grad_norm': 1.5838326215744019, 'learning_rate': 1.724e-05, 'epoch': 32.8}
{'loss': 0.0927, 'grad_norm': 1.9333068132400513, 'learning_rate': 1.684e-05, 'epoch': 33.2}
{'loss': 0.0767, 'grad_norm': 1.7591512203216553, 'learning_rate': 1.644e-05, 'epoch': 33.6}
{'loss': 0.1021, 'grad_norm': 1.4787613153457642, 'learning_rate': 1.604e-05, 'epoch': 34.0}
{'loss': 0.0761, 'grad_norm': 1.3619977235794067, 'learning_rate': 1.5640000000000003e-05, 'epoch': 34.4}
{'loss': 0.0818, 'grad_norm': 1.7517337799072266, 'learning_rate': 1.5240000000000001e-05, 'epoch': 34.8}
{'loss': 0.074, 'grad_norm': 1.9609508514404297, 'learning_rate': 1.4840000000000002e-05, 'epoch': 35.2}
{'loss': 0.0894, 'grad_norm': 2.2559664249420166, 'learning_rate': 1.444e-05, 'epoch': 35.6}
{'loss': 0.0782, 'grad_norm': 1.670701503753662, 'learning_rate': 1.4040000000000001e-05, 'epoch': 36.0}
{'loss': 0.073, 'grad_norm': 1.398226022720337, 'learning_rate': 1.364e-05, 'epoch': 36.4}
{'loss': 0.0896, 'grad_norm': 1.5571714639663696, 'learning_rate': 1.324e-05, 'epoch': 36.8}
{'loss': 0.0755, 'grad_norm': 1.7740979194641113, 'learning_rate': 1.2839999999999999e-05, 'epoch': 37.2}
{'loss': 0.0706, 'grad_norm': 2.043457269668579, 'learning_rate': 1.244e-05, 'epoch': 37.6}
{'loss': 0.0814, 'grad_norm': 2.0574607849121094, 'learning_rate': 1.204e-05, 'epoch': 38.0}
{'loss': 0.0766, 'grad_norm': 1.8064113855361938, 'learning_rate': 1.164e-05, 'epoch': 38.4}
{'loss': 0.0712, 'grad_norm': 1.490864634513855, 'learning_rate': 1.124e-05, 'epoch': 38.8}
{'loss': 0.0731, 'grad_norm': 1.515637755393982, 'learning_rate': 1.084e-05, 'epoch': 39.2}
{'loss': 0.0757, 'grad_norm': 1.4713002443313599, 'learning_rate': 1.0440000000000002e-05, 'epoch': 39.6}
{'loss': 0.0764, 'grad_norm': 1.4397412538528442, 'learning_rate': 1.004e-05, 'epoch': 40.0}
{'loss': 0.0746, 'grad_norm': 1.5969970226287842, 'learning_rate': 9.640000000000001e-06, 'epoch': 40.4}
{'loss': 0.0813, 'grad_norm': 1.965286374092102, 'learning_rate': 9.24e-06, 'epoch': 40.8}
{'loss': 0.0642, 'grad_norm': 1.5779657363891602, 'learning_rate': 8.840000000000002e-06, 'epoch': 41.2}
{'loss': 0.0733, 'grad_norm': 1.9835604429244995, 'learning_rate': 8.44e-06, 'epoch': 41.6}
{'loss': 0.0716, 'grad_norm': 1.4981633424758911, 'learning_rate': 8.040000000000001e-06, 'epoch': 42.0}
{'loss': 0.0919, 'grad_norm': 2.304988145828247, 'learning_rate': 7.64e-06, 'epoch': 42.4}
{'loss': 0.0681, 'grad_norm': 1.5504391193389893, 'learning_rate': 7.240000000000001e-06, 'epoch': 42.8}
{'loss': 0.0751, 'grad_norm': 2.2321019172668457, 'learning_rate': 6.840000000000001e-06, 'epoch': 43.2}
{'loss': 0.0707, 'grad_norm': 1.679044485092163, 'learning_rate': 6.44e-06, 'epoch': 43.6}
{'loss': 0.07, 'grad_norm': 1.2669787406921387, 'learning_rate': 6.040000000000001e-06, 'epoch': 44.0}
{'loss': 0.0696, 'grad_norm': 1.341605305671692, 'learning_rate': 5.64e-06, 'epoch': 44.4}
{'loss': 0.0678, 'grad_norm': 3.0545687675476074, 'learning_rate': 5.240000000000001e-06, 'epoch': 44.8}
{'loss': 0.064, 'grad_norm': 1.1488358974456787, 'learning_rate': 4.84e-06, 'epoch': 45.2}
{'loss': 0.0714, 'grad_norm': 1.9329595565795898, 'learning_rate': 4.440000000000001e-06, 'epoch': 45.6}
{'loss': 0.0669, 'grad_norm': 3.0172698497772217, 'learning_rate': 4.04e-06, 'epoch': 46.0}
{'loss': 0.069, 'grad_norm': 1.3614625930786133, 'learning_rate': 3.6400000000000003e-06, 'epoch': 46.4}
{'loss': 0.0628, 'grad_norm': 1.2031513452529907, 'learning_rate': 3.24e-06, 'epoch': 46.8}
{'loss': 0.0683, 'grad_norm': 1.6366058588027954, 'learning_rate': 2.8400000000000003e-06, 'epoch': 47.2}
{'loss': 0.0622, 'grad_norm': 1.4781434535980225, 'learning_rate': 2.4400000000000004e-06, 'epoch': 47.6}
{'loss': 0.0657, 'grad_norm': 2.178818941116333, 'learning_rate': 2.0400000000000004e-06, 'epoch': 48.0}
{'loss': 0.064, 'grad_norm': 1.4769632816314697, 'learning_rate': 1.6400000000000002e-06, 'epoch': 48.4}
{'loss': 0.0714, 'grad_norm': 1.749242901802063, 'learning_rate': 1.24e-06, 'epoch': 48.8}
{'loss': 0.0672, 'grad_norm': 1.576164722442627, 'learning_rate': 8.4e-07, 'epoch': 49.2}
{'loss': 0.0655, 'grad_norm': 1.6402934789657593, 'learning_rate': 4.4e-07, 'epoch': 49.6}
{'loss': 0.0615, 'grad_norm': 1.2954926490783691, 'learning_rate': 4e-08, 'epoch': 50.0}
{'train_runtime': 107.5, 'train_samples_per_second': 46.512, 'train_steps_per_second': 11.628, 'train_loss': 0.6155584821224213, 'epoch': 50.0}
100% 1250/1250 [01:47<00:00, 11.63it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:24<00:00,  1.02s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 53.12it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.4154
  Std compression: 1.8397

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.9797
  Std compression: 1.5286

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.4551
  Std compression: 1.4694

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.8269
  Std compression: 1.4206

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.1915
  Std compression: 1.4010

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.4643
  Std compression: 1.3687

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.7929
  Std compression: 1.3856

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.1471
  Std compression: 1.4027

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.5389
  Std compression: 1.4496

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.9736
  Std compression: 1.4451

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4317
  Std compression: 1.5962

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = -0.0328, p = 6.4467e-01
  Memorized sequences: mean = -5.3877, std = 0.2198, n = 100
  Novel sequences: mean = -5.3740, std = 0.1993, n = 100
  Difference: -0.0138
  T-test: t = -0.4619, p = 6.4464e-01
  ✗ WEAK: No significant correlation
Layer 0: r=-0.033, p=6.45e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = -0.0336, p = 6.3687e-01
  Memorized sequences: mean = -5.9569, std = 0.1998, n = 100
  Novel sequences: mean = -5.9445, std = 0.1673, n = 100
  Difference: -0.0124
  T-test: t = -0.4728, p = 6.3686e-01
  ✗ WEAK: No significant correlation
Layer 1: r=-0.034, p=6.37e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = -0.0475, p = 5.0462e-01
  Memorized sequences: mean = -6.4374, std = 0.2039, n = 100
  Novel sequences: mean = -6.4196, std = 0.1686, n = 100
  Difference: -0.0178
  T-test: t = -0.6684, p = 5.0463e-01
  ✗ WEAK: No significant correlation
Layer 2: r=-0.047, p=5.05e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0556, p = 4.3430e-01
  Memorized sequences: mean = -6.8113, std = 0.2046, n = 100
  Novel sequences: mean = -6.7907, std = 0.1624, n = 100
  Difference: -0.0206
  T-test: t = -0.7834, p = 4.3430e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.056, p=4.34e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = -0.0335, p = 6.3731e-01
  Memorized sequences: mean = -7.1671, std = 0.2091, n = 100
  Novel sequences: mean = -7.1548, std = 0.1528, n = 100
  Difference: -0.0123
  T-test: t = -0.4722, p = 6.3732e-01
  ✗ WEAK: No significant correlation
Layer 4: r=-0.034, p=6.37e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = -0.0636, p = 3.7131e-01
  Memorized sequences: mean = -7.4487, std = 0.2014, n = 100
  Novel sequences: mean = -7.4262, std = 0.1480, n = 100
  Difference: -0.0225
  T-test: t = -0.8961, p = 3.7130e-01
  ✗ WEAK: No significant correlation
Layer 5: r=-0.064, p=3.71e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = -0.0695, p = 3.2798e-01
  Memorized sequences: mean = -7.7764, std = 0.2086, n = 100
  Novel sequences: mean = -7.7511, std = 0.1486, n = 100
  Difference: -0.0252
  T-test: t = -0.9806, p = 3.2799e-01
  ✗ WEAK: No significant correlation
Layer 6: r=-0.070, p=3.28e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.1132, p = 1.1040e-01
  Memorized sequences: mean = -8.1410, std = 0.2092, n = 100
  Novel sequences: mean = -8.0994, std = 0.1512, n = 100
  Difference: -0.0416
  T-test: t = -1.6036, p = 1.1039e-01
  ✗ WEAK: No significant correlation
Layer 7: r=-0.113, p=1.10e-01

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.1155, p = 1.0333e-01
  Memorized sequences: mean = -8.5310, std = 0.2250, n = 100
  Novel sequences: mean = -8.4857, std = 0.1588, n = 100
  Difference: -0.0453
  T-test: t = -1.6365, p = 1.0333e-01
  ✗ WEAK: No significant correlation
Layer 8: r=-0.116, p=1.03e-01

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.1456, p = 3.9648e-02
  Memorized sequences: mean = -8.9711, std = 0.2247, n = 100
  Novel sequences: mean = -8.9142, std = 0.1555, n = 100
  Difference: -0.0569
  T-test: t = -2.0711, p = 3.9648e-02
  ✗ WEAK: No significant correlation
Layer 9: r=-0.146, p=3.96e-02

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = -0.0554, p = 4.3609e-01
  Memorized sequences: mean = -8.4269, std = 0.2088, n = 100
  Novel sequences: mean = -8.4059, std = 0.1674, n = 100
  Difference: -0.0210
  T-test: t = -0.7804, p = 4.3608e-01
  ✗ WEAK: No significant correlation
Layer 10: r=-0.055, p=4.36e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

======================================================================
CHECKPOINT: 100 EPOCHS
======================================================================

Loading base model...
CUDA is available. Using GPU.
Loading model: gpt2
Using device: cuda
Warning: Model is on cuda:0 but requested cuda
GPU: NVIDIA L4
Memory: 23.80 GB
Model loaded successfully!
Fine-tuning for 100 epochs...

Fine-tuning model on 100 texts...
Device: cuda
Epochs: 100, Learning rate: 5e-05, Batch size: 4
Preparing training dataset from 100 texts...
Prepared 100 training sequences

Starting training...
{'loss': 4.046, 'grad_norm': 11.625048637390137, 'learning_rate': 4.982e-05, 'epoch': 0.4}
{'loss': 3.885, 'grad_norm': 14.701804161071777, 'learning_rate': 4.962e-05, 'epoch': 0.8}
{'loss': 3.4897, 'grad_norm': 13.195632934570312, 'learning_rate': 4.942e-05, 'epoch': 1.2}
{'loss': 3.2581, 'grad_norm': 10.85831356048584, 'learning_rate': 4.9220000000000006e-05, 'epoch': 1.6}
{'loss': 3.2559, 'grad_norm': 14.890364646911621, 'learning_rate': 4.902e-05, 'epoch': 2.0}
{'loss': 2.8321, 'grad_norm': 12.14074420928955, 'learning_rate': 4.8820000000000004e-05, 'epoch': 2.4}
{'loss': 2.8151, 'grad_norm': 12.714224815368652, 'learning_rate': 4.8620000000000005e-05, 'epoch': 2.8}
{'loss': 2.6714, 'grad_norm': 13.566851615905762, 'learning_rate': 4.842000000000001e-05, 'epoch': 3.2}
{'loss': 2.4891, 'grad_norm': 12.826933860778809, 'learning_rate': 4.822e-05, 'epoch': 3.6}
{'loss': 2.4067, 'grad_norm': 13.577692985534668, 'learning_rate': 4.8020000000000004e-05, 'epoch': 4.0}
{'loss': 2.1936, 'grad_norm': 11.978665351867676, 'learning_rate': 4.7820000000000006e-05, 'epoch': 4.4}
{'loss': 2.2191, 'grad_norm': 12.961098670959473, 'learning_rate': 4.762e-05, 'epoch': 4.8}
{'loss': 2.1201, 'grad_norm': 12.492825508117676, 'learning_rate': 4.742e-05, 'epoch': 5.2}
{'loss': 1.9434, 'grad_norm': 12.5870361328125, 'learning_rate': 4.7220000000000005e-05, 'epoch': 5.6}
{'loss': 1.9346, 'grad_norm': 13.396623611450195, 'learning_rate': 4.702e-05, 'epoch': 6.0}
{'loss': 1.7316, 'grad_norm': 10.995783805847168, 'learning_rate': 4.682e-05, 'epoch': 6.4}
{'loss': 1.7105, 'grad_norm': 11.817777633666992, 'learning_rate': 4.6620000000000004e-05, 'epoch': 6.8}
{'loss': 1.5653, 'grad_norm': 12.065766334533691, 'learning_rate': 4.642e-05, 'epoch': 7.2}
{'loss': 1.5053, 'grad_norm': 10.681685447692871, 'learning_rate': 4.622e-05, 'epoch': 7.6}
{'loss': 1.5076, 'grad_norm': 11.934691429138184, 'learning_rate': 4.602e-05, 'epoch': 8.0}
{'loss': 1.3267, 'grad_norm': 11.191509246826172, 'learning_rate': 4.5820000000000005e-05, 'epoch': 8.4}
{'loss': 1.2593, 'grad_norm': 13.63664436340332, 'learning_rate': 4.562e-05, 'epoch': 8.8}
{'loss': 1.1722, 'grad_norm': 8.552475929260254, 'learning_rate': 4.542e-05, 'epoch': 9.2}
{'loss': 1.0775, 'grad_norm': 7.76771879196167, 'learning_rate': 4.5220000000000004e-05, 'epoch': 9.6}
{'loss': 1.1059, 'grad_norm': 9.221590042114258, 'learning_rate': 4.502e-05, 'epoch': 10.0}
{'loss': 0.9228, 'grad_norm': 8.58608627319336, 'learning_rate': 4.482e-05, 'epoch': 10.4}
{'loss': 0.9071, 'grad_norm': 9.563395500183105, 'learning_rate': 4.462e-05, 'epoch': 10.8}
{'loss': 0.8777, 'grad_norm': 8.65479564666748, 'learning_rate': 4.442e-05, 'epoch': 11.2}
{'loss': 0.7086, 'grad_norm': 7.145942687988281, 'learning_rate': 4.422e-05, 'epoch': 11.6}
{'loss': 0.7652, 'grad_norm': 8.137938499450684, 'learning_rate': 4.402e-05, 'epoch': 12.0}
{'loss': 0.6079, 'grad_norm': 6.404302597045898, 'learning_rate': 4.382e-05, 'epoch': 12.4}
{'loss': 0.6043, 'grad_norm': 6.323029518127441, 'learning_rate': 4.362e-05, 'epoch': 12.8}
{'loss': 0.5609, 'grad_norm': 6.8834710121154785, 'learning_rate': 4.342e-05, 'epoch': 13.2}
{'loss': 0.5045, 'grad_norm': 6.525638580322266, 'learning_rate': 4.3219999999999996e-05, 'epoch': 13.6}
{'loss': 0.519, 'grad_norm': 6.359220027923584, 'learning_rate': 4.3020000000000005e-05, 'epoch': 14.0}
{'loss': 0.4368, 'grad_norm': 4.312690734863281, 'learning_rate': 4.282000000000001e-05, 'epoch': 14.4}
{'loss': 0.4036, 'grad_norm': 5.00471830368042, 'learning_rate': 4.262e-05, 'epoch': 14.8}
{'loss': 0.404, 'grad_norm': 4.287121295928955, 'learning_rate': 4.2420000000000004e-05, 'epoch': 15.2}
{'loss': 0.358, 'grad_norm': 5.041408538818359, 'learning_rate': 4.2220000000000006e-05, 'epoch': 15.6}
{'loss': 0.3505, 'grad_norm': 5.0138726234436035, 'learning_rate': 4.202e-05, 'epoch': 16.0}
{'loss': 0.2843, 'grad_norm': 4.443085193634033, 'learning_rate': 4.182e-05, 'epoch': 16.4}
{'loss': 0.3103, 'grad_norm': 3.8931453227996826, 'learning_rate': 4.1620000000000005e-05, 'epoch': 16.8}
{'loss': 0.2932, 'grad_norm': 4.601790904998779, 'learning_rate': 4.142000000000001e-05, 'epoch': 17.2}
{'loss': 0.2457, 'grad_norm': 3.5714144706726074, 'learning_rate': 4.122e-05, 'epoch': 17.6}
{'loss': 0.2611, 'grad_norm': 4.014108657836914, 'learning_rate': 4.1020000000000004e-05, 'epoch': 18.0}
{'loss': 0.2097, 'grad_norm': 3.5391881465911865, 'learning_rate': 4.0820000000000006e-05, 'epoch': 18.4}
{'loss': 0.2319, 'grad_norm': 4.324643611907959, 'learning_rate': 4.062e-05, 'epoch': 18.8}
{'loss': 0.2174, 'grad_norm': 2.8523988723754883, 'learning_rate': 4.042e-05, 'epoch': 19.2}
{'loss': 0.185, 'grad_norm': 3.0483884811401367, 'learning_rate': 4.0220000000000005e-05, 'epoch': 19.6}
{'loss': 0.1953, 'grad_norm': 3.575985908508301, 'learning_rate': 4.002e-05, 'epoch': 20.0}
{'loss': 0.1644, 'grad_norm': 3.399111747741699, 'learning_rate': 3.982e-05, 'epoch': 20.4}
{'loss': 0.1857, 'grad_norm': 3.912780284881592, 'learning_rate': 3.9620000000000004e-05, 'epoch': 20.8}
{'loss': 0.1698, 'grad_norm': 3.1736056804656982, 'learning_rate': 3.942e-05, 'epoch': 21.2}
{'loss': 0.1613, 'grad_norm': 2.386608362197876, 'learning_rate': 3.922e-05, 'epoch': 21.6}
{'loss': 0.157, 'grad_norm': 3.030945062637329, 'learning_rate': 3.902e-05, 'epoch': 22.0}
{'loss': 0.1362, 'grad_norm': 2.999037027359009, 'learning_rate': 3.882e-05, 'epoch': 22.4}
{'loss': 0.1388, 'grad_norm': 2.1321349143981934, 'learning_rate': 3.862e-05, 'epoch': 22.8}
{'loss': 0.138, 'grad_norm': 3.120335340499878, 'learning_rate': 3.842e-05, 'epoch': 23.2}
{'loss': 0.1454, 'grad_norm': 3.1447885036468506, 'learning_rate': 3.822e-05, 'epoch': 23.6}
{'loss': 0.1367, 'grad_norm': 2.805820941925049, 'learning_rate': 3.802e-05, 'epoch': 24.0}
{'loss': 0.118, 'grad_norm': 2.057387351989746, 'learning_rate': 3.782e-05, 'epoch': 24.4}
{'loss': 0.1179, 'grad_norm': 3.6125054359436035, 'learning_rate': 3.762e-05, 'epoch': 24.8}
{'loss': 0.113, 'grad_norm': 1.76924729347229, 'learning_rate': 3.742e-05, 'epoch': 25.2}
{'loss': 0.119, 'grad_norm': 2.3436899185180664, 'learning_rate': 3.722e-05, 'epoch': 25.6}
{'loss': 0.1206, 'grad_norm': 2.4736597537994385, 'learning_rate': 3.702e-05, 'epoch': 26.0}
{'loss': 0.0981, 'grad_norm': 1.8422633409500122, 'learning_rate': 3.682e-05, 'epoch': 26.4}
{'loss': 0.0989, 'grad_norm': 2.309433698654175, 'learning_rate': 3.6620000000000005e-05, 'epoch': 26.8}
{'loss': 0.1078, 'grad_norm': 1.6713041067123413, 'learning_rate': 3.642000000000001e-05, 'epoch': 27.2}
{'loss': 0.0986, 'grad_norm': 2.3178939819335938, 'learning_rate': 3.622e-05, 'epoch': 27.6}
{'loss': 0.1039, 'grad_norm': 3.345710039138794, 'learning_rate': 3.6020000000000004e-05, 'epoch': 28.0}
{'loss': 0.0952, 'grad_norm': 2.3567049503326416, 'learning_rate': 3.5820000000000006e-05, 'epoch': 28.4}
{'loss': 0.0899, 'grad_norm': 2.5189220905303955, 'learning_rate': 3.562e-05, 'epoch': 28.8}
{'loss': 0.0894, 'grad_norm': 2.3141393661499023, 'learning_rate': 3.542e-05, 'epoch': 29.2}
{'loss': 0.0838, 'grad_norm': 1.863237738609314, 'learning_rate': 3.5220000000000005e-05, 'epoch': 29.6}
{'loss': 0.0842, 'grad_norm': 1.828768014907837, 'learning_rate': 3.502e-05, 'epoch': 30.0}
{'loss': 0.0832, 'grad_norm': 2.091635227203369, 'learning_rate': 3.482e-05, 'epoch': 30.4}
{'loss': 0.0818, 'grad_norm': 1.403113603591919, 'learning_rate': 3.4620000000000004e-05, 'epoch': 30.8}
{'loss': 0.0818, 'grad_norm': 1.177975058555603, 'learning_rate': 3.442e-05, 'epoch': 31.2}
{'loss': 0.0674, 'grad_norm': 2.0249135494232178, 'learning_rate': 3.422e-05, 'epoch': 31.6}
{'loss': 0.089, 'grad_norm': 2.445465087890625, 'learning_rate': 3.402e-05, 'epoch': 32.0}
{'loss': 0.0681, 'grad_norm': 1.4359887838363647, 'learning_rate': 3.3820000000000005e-05, 'epoch': 32.4}
{'loss': 0.0791, 'grad_norm': 1.5252478122711182, 'learning_rate': 3.362e-05, 'epoch': 32.8}
{'loss': 0.0762, 'grad_norm': 1.6918739080429077, 'learning_rate': 3.342e-05, 'epoch': 33.2}
{'loss': 0.0634, 'grad_norm': 1.7407664060592651, 'learning_rate': 3.3220000000000004e-05, 'epoch': 33.6}
{'loss': 0.0862, 'grad_norm': 1.1046100854873657, 'learning_rate': 3.302e-05, 'epoch': 34.0}
{'loss': 0.0628, 'grad_norm': 1.1296018362045288, 'learning_rate': 3.282e-05, 'epoch': 34.4}
{'loss': 0.0683, 'grad_norm': 1.8066792488098145, 'learning_rate': 3.262e-05, 'epoch': 34.8}
{'loss': 0.063, 'grad_norm': 1.669582724571228, 'learning_rate': 3.242e-05, 'epoch': 35.2}
{'loss': 0.0749, 'grad_norm': 2.203899383544922, 'learning_rate': 3.222e-05, 'epoch': 35.6}
{'loss': 0.0665, 'grad_norm': 1.497023582458496, 'learning_rate': 3.202e-05, 'epoch': 36.0}
{'loss': 0.0616, 'grad_norm': 1.2323216199874878, 'learning_rate': 3.182e-05, 'epoch': 36.4}
{'loss': 0.068, 'grad_norm': 1.2973253726959229, 'learning_rate': 3.162e-05, 'epoch': 36.8}
{'loss': 0.064, 'grad_norm': 1.433632254600525, 'learning_rate': 3.142e-05, 'epoch': 37.2}
{'loss': 0.0555, 'grad_norm': 1.6072293519973755, 'learning_rate': 3.122e-05, 'epoch': 37.6}
{'loss': 0.07, 'grad_norm': 2.406097173690796, 'learning_rate': 3.102e-05, 'epoch': 38.0}
{'loss': 0.0616, 'grad_norm': 1.7610214948654175, 'learning_rate': 3.082e-05, 'epoch': 38.4}
{'loss': 0.0593, 'grad_norm': 0.9021198153495789, 'learning_rate': 3.062e-05, 'epoch': 38.8}
{'loss': 0.062, 'grad_norm': 1.4208425283432007, 'learning_rate': 3.0420000000000004e-05, 'epoch': 39.2}
{'loss': 0.0607, 'grad_norm': 1.5289708375930786, 'learning_rate': 3.0220000000000005e-05, 'epoch': 39.6}
{'loss': 0.0616, 'grad_norm': 1.242864727973938, 'learning_rate': 3.0020000000000004e-05, 'epoch': 40.0}
{'loss': 0.06, 'grad_norm': 1.438902497291565, 'learning_rate': 2.9820000000000002e-05, 'epoch': 40.4}
{'loss': 0.0682, 'grad_norm': 1.6123459339141846, 'learning_rate': 2.9620000000000004e-05, 'epoch': 40.8}
{'loss': 0.0541, 'grad_norm': 1.3205057382583618, 'learning_rate': 2.9420000000000003e-05, 'epoch': 41.2}
{'loss': 0.0609, 'grad_norm': 1.923356294631958, 'learning_rate': 2.922e-05, 'epoch': 41.6}
{'loss': 0.0584, 'grad_norm': 1.1467149257659912, 'learning_rate': 2.9020000000000003e-05, 'epoch': 42.0}
{'loss': 0.0693, 'grad_norm': 2.0149192810058594, 'learning_rate': 2.8820000000000002e-05, 'epoch': 42.4}
{'loss': 0.0553, 'grad_norm': 1.3999098539352417, 'learning_rate': 2.8620000000000004e-05, 'epoch': 42.8}
{'loss': 0.0544, 'grad_norm': 1.2928467988967896, 'learning_rate': 2.8420000000000002e-05, 'epoch': 43.2}
{'loss': 0.0595, 'grad_norm': 1.4919482469558716, 'learning_rate': 2.822e-05, 'epoch': 43.6}
{'loss': 0.0623, 'grad_norm': 1.0003682374954224, 'learning_rate': 2.8020000000000003e-05, 'epoch': 44.0}
{'loss': 0.0551, 'grad_norm': 0.8919596672058105, 'learning_rate': 2.782e-05, 'epoch': 44.4}
{'loss': 0.0537, 'grad_norm': 2.2100462913513184, 'learning_rate': 2.762e-05, 'epoch': 44.8}
{'loss': 0.0522, 'grad_norm': 2.369560718536377, 'learning_rate': 2.7420000000000002e-05, 'epoch': 45.2}
{'loss': 0.0567, 'grad_norm': 1.2214018106460571, 'learning_rate': 2.722e-05, 'epoch': 45.6}
{'loss': 0.0505, 'grad_norm': 1.460249423980713, 'learning_rate': 2.7020000000000002e-05, 'epoch': 46.0}
{'loss': 0.0547, 'grad_norm': 1.3910894393920898, 'learning_rate': 2.682e-05, 'epoch': 46.4}
{'loss': 0.0518, 'grad_norm': 1.0643296241760254, 'learning_rate': 2.662e-05, 'epoch': 46.8}
{'loss': 0.0538, 'grad_norm': 1.1585817337036133, 'learning_rate': 2.642e-05, 'epoch': 47.2}
{'loss': 0.0494, 'grad_norm': 1.8082618713378906, 'learning_rate': 2.622e-05, 'epoch': 47.6}
{'loss': 0.0545, 'grad_norm': 2.2721669673919678, 'learning_rate': 2.602e-05, 'epoch': 48.0}
{'loss': 0.0465, 'grad_norm': 0.9748753309249878, 'learning_rate': 2.582e-05, 'epoch': 48.4}
{'loss': 0.058, 'grad_norm': 1.3767067193984985, 'learning_rate': 2.562e-05, 'epoch': 48.8}
{'loss': 0.0497, 'grad_norm': 1.1965233087539673, 'learning_rate': 2.542e-05, 'epoch': 49.2}
{'loss': 0.05, 'grad_norm': 1.5091899633407593, 'learning_rate': 2.522e-05, 'epoch': 49.6}
{'loss': 0.0514, 'grad_norm': 1.4087073802947998, 'learning_rate': 2.5019999999999998e-05, 'epoch': 50.0}
{'loss': 0.0419, 'grad_norm': 0.9685668349266052, 'learning_rate': 2.4820000000000003e-05, 'epoch': 50.4}
{'loss': 0.0528, 'grad_norm': 1.1231306791305542, 'learning_rate': 2.462e-05, 'epoch': 50.8}
{'loss': 0.0467, 'grad_norm': 1.1289945840835571, 'learning_rate': 2.442e-05, 'epoch': 51.2}
{'loss': 0.0502, 'grad_norm': 1.5687140226364136, 'learning_rate': 2.4220000000000002e-05, 'epoch': 51.6}
{'loss': 0.0492, 'grad_norm': 0.9440814852714539, 'learning_rate': 2.402e-05, 'epoch': 52.0}
{'loss': 0.0472, 'grad_norm': 1.2255148887634277, 'learning_rate': 2.3820000000000002e-05, 'epoch': 52.4}
{'loss': 0.0529, 'grad_norm': 1.3049148321151733, 'learning_rate': 2.362e-05, 'epoch': 52.8}
{'loss': 0.05, 'grad_norm': 1.133584976196289, 'learning_rate': 2.342e-05, 'epoch': 53.2}
{'loss': 0.0488, 'grad_norm': 1.2949931621551514, 'learning_rate': 2.322e-05, 'epoch': 53.6}
{'loss': 0.0453, 'grad_norm': 1.179373860359192, 'learning_rate': 2.302e-05, 'epoch': 54.0}
{'loss': 0.0392, 'grad_norm': 0.8470203280448914, 'learning_rate': 2.282e-05, 'epoch': 54.4}
{'loss': 0.0573, 'grad_norm': 0.6174209713935852, 'learning_rate': 2.2620000000000004e-05, 'epoch': 54.8}
{'loss': 0.0425, 'grad_norm': 0.5829477906227112, 'learning_rate': 2.2420000000000002e-05, 'epoch': 55.2}
{'loss': 0.0375, 'grad_norm': 1.1270989179611206, 'learning_rate': 2.222e-05, 'epoch': 55.6}
{'loss': 0.0587, 'grad_norm': 1.3716694116592407, 'learning_rate': 2.2020000000000003e-05, 'epoch': 56.0}
{'loss': 0.0417, 'grad_norm': 1.614318609237671, 'learning_rate': 2.182e-05, 'epoch': 56.4}
{'loss': 0.0482, 'grad_norm': 4.270232200622559, 'learning_rate': 2.162e-05, 'epoch': 56.8}
{'loss': 0.0392, 'grad_norm': 1.509268045425415, 'learning_rate': 2.142e-05, 'epoch': 57.2}
{'loss': 0.0453, 'grad_norm': 1.5377691984176636, 'learning_rate': 2.122e-05, 'epoch': 57.6}
{'loss': 0.0512, 'grad_norm': 1.6478742361068726, 'learning_rate': 2.1020000000000002e-05, 'epoch': 58.0}
{'loss': 0.0459, 'grad_norm': 1.1944037675857544, 'learning_rate': 2.082e-05, 'epoch': 58.4}
{'loss': 0.0454, 'grad_norm': 1.9633550643920898, 'learning_rate': 2.062e-05, 'epoch': 58.8}
{'loss': 0.0435, 'grad_norm': 0.8910992741584778, 'learning_rate': 2.042e-05, 'epoch': 59.2}
{'loss': 0.0433, 'grad_norm': 1.4562753438949585, 'learning_rate': 2.022e-05, 'epoch': 59.6}
{'loss': 0.0444, 'grad_norm': 1.0328092575073242, 'learning_rate': 2.002e-05, 'epoch': 60.0}
{'loss': 0.0483, 'grad_norm': 0.6642444729804993, 'learning_rate': 1.982e-05, 'epoch': 60.4}
{'loss': 0.0493, 'grad_norm': 1.0932339429855347, 'learning_rate': 1.9620000000000002e-05, 'epoch': 60.8}
{'loss': 0.0425, 'grad_norm': 1.0352579355239868, 'learning_rate': 1.942e-05, 'epoch': 61.2}
{'loss': 0.0459, 'grad_norm': 1.058408260345459, 'learning_rate': 1.9220000000000002e-05, 'epoch': 61.6}
{'loss': 0.0477, 'grad_norm': 0.32506316900253296, 'learning_rate': 1.902e-05, 'epoch': 62.0}
{'loss': 0.0525, 'grad_norm': 1.0869004726409912, 'learning_rate': 1.8820000000000003e-05, 'epoch': 62.4}
{'loss': 0.0462, 'grad_norm': 1.0504053831100464, 'learning_rate': 1.862e-05, 'epoch': 62.8}
{'loss': 0.0445, 'grad_norm': 1.0235562324523926, 'learning_rate': 1.842e-05, 'epoch': 63.2}
{'loss': 0.0402, 'grad_norm': 1.137357234954834, 'learning_rate': 1.8220000000000002e-05, 'epoch': 63.6}
{'loss': 0.0408, 'grad_norm': 1.7342274188995361, 'learning_rate': 1.802e-05, 'epoch': 64.0}
{'loss': 0.0442, 'grad_norm': 1.2967129945755005, 'learning_rate': 1.7820000000000002e-05, 'epoch': 64.4}
{'loss': 0.0448, 'grad_norm': 1.971392035484314, 'learning_rate': 1.762e-05, 'epoch': 64.8}
{'loss': 0.0386, 'grad_norm': 0.8836560249328613, 'learning_rate': 1.742e-05, 'epoch': 65.2}
{'loss': 0.0408, 'grad_norm': 0.9483374953269958, 'learning_rate': 1.722e-05, 'epoch': 65.6}
{'loss': 0.0391, 'grad_norm': 1.2002074718475342, 'learning_rate': 1.702e-05, 'epoch': 66.0}
{'loss': 0.0388, 'grad_norm': 1.058358907699585, 'learning_rate': 1.6819999999999998e-05, 'epoch': 66.4}
{'loss': 0.0497, 'grad_norm': 1.3028838634490967, 'learning_rate': 1.662e-05, 'epoch': 66.8}
{'loss': 0.0416, 'grad_norm': 1.5436692237854004, 'learning_rate': 1.6420000000000002e-05, 'epoch': 67.2}
{'loss': 0.0358, 'grad_norm': 1.441016435623169, 'learning_rate': 1.622e-05, 'epoch': 67.6}
{'loss': 0.041, 'grad_norm': 0.8280672430992126, 'learning_rate': 1.6020000000000002e-05, 'epoch': 68.0}
{'loss': 0.0429, 'grad_norm': 1.6849737167358398, 'learning_rate': 1.582e-05, 'epoch': 68.4}
{'loss': 0.0385, 'grad_norm': 2.1101715564727783, 'learning_rate': 1.5620000000000003e-05, 'epoch': 68.8}
{'loss': 0.0317, 'grad_norm': 0.9623020887374878, 'learning_rate': 1.542e-05, 'epoch': 69.2}
{'loss': 0.0385, 'grad_norm': 0.9208893179893494, 'learning_rate': 1.5220000000000002e-05, 'epoch': 69.6}
{'loss': 0.0418, 'grad_norm': 0.485329806804657, 'learning_rate': 1.502e-05, 'epoch': 70.0}
{'loss': 0.0419, 'grad_norm': 0.9451528191566467, 'learning_rate': 1.482e-05, 'epoch': 70.4}
{'loss': 0.0389, 'grad_norm': 0.7641094326972961, 'learning_rate': 1.462e-05, 'epoch': 70.8}
{'loss': 0.0348, 'grad_norm': 1.1479383707046509, 'learning_rate': 1.4420000000000001e-05, 'epoch': 71.2}
{'loss': 0.042, 'grad_norm': 1.417717695236206, 'learning_rate': 1.422e-05, 'epoch': 71.6}
{'loss': 0.0364, 'grad_norm': 1.212073802947998, 'learning_rate': 1.402e-05, 'epoch': 72.0}
{'loss': 0.0381, 'grad_norm': 1.3163775205612183, 'learning_rate': 1.382e-05, 'epoch': 72.4}
{'loss': 0.0434, 'grad_norm': 0.5735535025596619, 'learning_rate': 1.362e-05, 'epoch': 72.8}
{'loss': 0.0425, 'grad_norm': 1.24647855758667, 'learning_rate': 1.3420000000000002e-05, 'epoch': 73.2}
{'loss': 0.0408, 'grad_norm': 1.3358242511749268, 'learning_rate': 1.3220000000000002e-05, 'epoch': 73.6}
{'loss': 0.0345, 'grad_norm': 1.1599239110946655, 'learning_rate': 1.3020000000000002e-05, 'epoch': 74.0}
{'loss': 0.0492, 'grad_norm': 0.8854027390480042, 'learning_rate': 1.2820000000000001e-05, 'epoch': 74.4}
{'loss': 0.0448, 'grad_norm': 1.5859758853912354, 'learning_rate': 1.2620000000000001e-05, 'epoch': 74.8}
{'loss': 0.0385, 'grad_norm': 1.4699419736862183, 'learning_rate': 1.2420000000000001e-05, 'epoch': 75.2}
{'loss': 0.0506, 'grad_norm': 1.202636957168579, 'learning_rate': 1.2220000000000002e-05, 'epoch': 75.6}
{'loss': 0.0445, 'grad_norm': 0.958358645439148, 'learning_rate': 1.202e-05, 'epoch': 76.0}
{'loss': 0.0419, 'grad_norm': 1.3771976232528687, 'learning_rate': 1.182e-05, 'epoch': 76.4}
{'loss': 0.0436, 'grad_norm': 1.86612069606781, 'learning_rate': 1.162e-05, 'epoch': 76.8}
{'loss': 0.0377, 'grad_norm': 1.4671729803085327, 'learning_rate': 1.142e-05, 'epoch': 77.2}
{'loss': 0.0411, 'grad_norm': 1.360594630241394, 'learning_rate': 1.122e-05, 'epoch': 77.6}
{'loss': 0.0356, 'grad_norm': 0.7865917086601257, 'learning_rate': 1.1020000000000001e-05, 'epoch': 78.0}
{'loss': 0.0437, 'grad_norm': 0.9059122204780579, 'learning_rate': 1.0820000000000001e-05, 'epoch': 78.4}
{'loss': 0.0419, 'grad_norm': 1.1841672658920288, 'learning_rate': 1.062e-05, 'epoch': 78.8}
{'loss': 0.0353, 'grad_norm': 0.6696662306785583, 'learning_rate': 1.042e-05, 'epoch': 79.2}
{'loss': 0.0423, 'grad_norm': 1.105785608291626, 'learning_rate': 1.022e-05, 'epoch': 79.6}
{'loss': 0.0443, 'grad_norm': 1.181398630142212, 'learning_rate': 1.002e-05, 'epoch': 80.0}
{'loss': 0.0464, 'grad_norm': 0.9840553998947144, 'learning_rate': 9.820000000000001e-06, 'epoch': 80.4}
{'loss': 0.0291, 'grad_norm': 1.2702460289001465, 'learning_rate': 9.62e-06, 'epoch': 80.8}
{'loss': 0.0447, 'grad_norm': 0.5000290870666504, 'learning_rate': 9.420000000000001e-06, 'epoch': 81.2}
{'loss': 0.0407, 'grad_norm': 1.1587661504745483, 'learning_rate': 9.220000000000002e-06, 'epoch': 81.6}
{'loss': 0.0418, 'grad_norm': 1.8176573514938354, 'learning_rate': 9.02e-06, 'epoch': 82.0}
{'loss': 0.0327, 'grad_norm': 1.1070829629898071, 'learning_rate': 8.82e-06, 'epoch': 82.4}
{'loss': 0.0409, 'grad_norm': 0.7375321984291077, 'learning_rate': 8.62e-06, 'epoch': 82.8}
{'loss': 0.0357, 'grad_norm': 1.4461230039596558, 'learning_rate': 8.42e-06, 'epoch': 83.2}
{'loss': 0.043, 'grad_norm': 0.6036879420280457, 'learning_rate': 8.22e-06, 'epoch': 83.6}
{'loss': 0.0473, 'grad_norm': 0.8947778940200806, 'learning_rate': 8.02e-06, 'epoch': 84.0}
{'loss': 0.0367, 'grad_norm': 1.1470470428466797, 'learning_rate': 7.820000000000001e-06, 'epoch': 84.4}
{'loss': 0.0383, 'grad_norm': 1.1325160264968872, 'learning_rate': 7.620000000000001e-06, 'epoch': 84.8}
{'loss': 0.0343, 'grad_norm': 1.7256629467010498, 'learning_rate': 7.420000000000001e-06, 'epoch': 85.2}
{'loss': 0.038, 'grad_norm': 1.1539512872695923, 'learning_rate': 7.22e-06, 'epoch': 85.6}
{'loss': 0.041, 'grad_norm': 1.0409680604934692, 'learning_rate': 7.0200000000000006e-06, 'epoch': 86.0}
{'loss': 0.0383, 'grad_norm': 1.0895805358886719, 'learning_rate': 6.82e-06, 'epoch': 86.4}
{'loss': 0.0351, 'grad_norm': 1.5177562236785889, 'learning_rate': 6.62e-06, 'epoch': 86.8}
{'loss': 0.0445, 'grad_norm': 1.370180606842041, 'learning_rate': 6.4199999999999995e-06, 'epoch': 87.2}
{'loss': 0.0385, 'grad_norm': 1.4548254013061523, 'learning_rate': 6.22e-06, 'epoch': 87.6}
{'loss': 0.038, 'grad_norm': 0.7362627387046814, 'learning_rate': 6.02e-06, 'epoch': 88.0}
{'loss': 0.0396, 'grad_norm': 1.2175804376602173, 'learning_rate': 5.82e-06, 'epoch': 88.4}
{'loss': 0.0326, 'grad_norm': 0.8591406941413879, 'learning_rate': 5.62e-06, 'epoch': 88.8}
{'loss': 0.0448, 'grad_norm': 0.9490706324577332, 'learning_rate': 5.42e-06, 'epoch': 89.2}
{'loss': 0.0293, 'grad_norm': 1.2840640544891357, 'learning_rate': 5.220000000000001e-06, 'epoch': 89.6}
{'loss': 0.0521, 'grad_norm': 1.2214899063110352, 'learning_rate': 5.02e-06, 'epoch': 90.0}
{'loss': 0.0473, 'grad_norm': 1.0604259967803955, 'learning_rate': 4.8200000000000004e-06, 'epoch': 90.4}
{'loss': 0.0356, 'grad_norm': 1.2513062953948975, 'learning_rate': 4.62e-06, 'epoch': 90.8}
{'loss': 0.0357, 'grad_norm': 1.3894243240356445, 'learning_rate': 4.420000000000001e-06, 'epoch': 91.2}
{'loss': 0.0401, 'grad_norm': 1.3678152561187744, 'learning_rate': 4.22e-06, 'epoch': 91.6}
{'loss': 0.0336, 'grad_norm': 1.1366357803344727, 'learning_rate': 4.0200000000000005e-06, 'epoch': 92.0}
{'loss': 0.0392, 'grad_norm': 1.615347146987915, 'learning_rate': 3.82e-06, 'epoch': 92.4}
{'loss': 0.0484, 'grad_norm': 1.1568588018417358, 'learning_rate': 3.6200000000000005e-06, 'epoch': 92.8}
{'loss': 0.0315, 'grad_norm': 1.0826587677001953, 'learning_rate': 3.4200000000000003e-06, 'epoch': 93.2}
{'loss': 0.0339, 'grad_norm': 0.8397637009620667, 'learning_rate': 3.22e-06, 'epoch': 93.6}
{'loss': 0.0429, 'grad_norm': 1.2470550537109375, 'learning_rate': 3.0200000000000003e-06, 'epoch': 94.0}
{'loss': 0.0343, 'grad_norm': 0.8060030341148376, 'learning_rate': 2.82e-06, 'epoch': 94.4}
{'loss': 0.0447, 'grad_norm': 1.2752799987792969, 'learning_rate': 2.6200000000000003e-06, 'epoch': 94.8}
{'loss': 0.0368, 'grad_norm': 0.8656448721885681, 'learning_rate': 2.42e-06, 'epoch': 95.2}
{'loss': 0.0316, 'grad_norm': 1.2445738315582275, 'learning_rate': 2.2200000000000003e-06, 'epoch': 95.6}
{'loss': 0.0307, 'grad_norm': 1.1716840267181396, 'learning_rate': 2.02e-06, 'epoch': 96.0}
{'loss': 0.0381, 'grad_norm': 0.8322794437408447, 'learning_rate': 1.8200000000000002e-06, 'epoch': 96.4}
{'loss': 0.0401, 'grad_norm': 1.0870615243911743, 'learning_rate': 1.62e-06, 'epoch': 96.8}
{'loss': 0.0422, 'grad_norm': 1.1641520261764526, 'learning_rate': 1.4200000000000002e-06, 'epoch': 97.2}
{'loss': 0.0408, 'grad_norm': 0.9865765571594238, 'learning_rate': 1.2200000000000002e-06, 'epoch': 97.6}
{'loss': 0.0341, 'grad_norm': 1.063857078552246, 'learning_rate': 1.0200000000000002e-06, 'epoch': 98.0}
{'loss': 0.0384, 'grad_norm': 1.0290380716323853, 'learning_rate': 8.200000000000001e-07, 'epoch': 98.4}
{'loss': 0.0292, 'grad_norm': 1.5601364374160767, 'learning_rate': 6.2e-07, 'epoch': 98.8}
{'loss': 0.032, 'grad_norm': 1.1636242866516113, 'learning_rate': 4.2e-07, 'epoch': 99.2}
{'loss': 0.0431, 'grad_norm': 0.8432602882385254, 'learning_rate': 2.2e-07, 'epoch': 99.6}
{'loss': 0.04, 'grad_norm': 1.5576529502868652, 'learning_rate': 2e-08, 'epoch': 100.0}
{'train_runtime': 214.8423, 'train_samples_per_second': 46.546, 'train_steps_per_second': 11.636, 'train_loss': 0.31309958058595655, 'epoch': 100.0}
100% 2500/2500 [03:34<00:00, 11.64it/s]
Fine-tuning complete!

Computing reproduction metrics...
Computing reproduction metrics: 100% 200/200 [03:28<00:00,  1.04s/it]

Extracting hidden states...
Extracting states: 100% 200/200 [00:03<00:00, 52.22it/s]

Computing compression scores...

Computing compression for layer 0 -> 1
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -5.4748
  Std compression: 1.8813

Computing compression for layer 1 -> 2
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.0489
  Std compression: 1.5481

Computing compression for layer 2 -> 3
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.5280
  Std compression: 1.4661

Computing compression for layer 3 -> 4
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -6.9024
  Std compression: 1.4322

Computing compression for layer 4 -> 5
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.2731
  Std compression: 1.4127

Computing compression for layer 5 -> 6
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.5395
  Std compression: 1.3887

Computing compression for layer 6 -> 7
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -7.8616
  Std compression: 1.3945

Computing compression for layer 7 -> 8
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.2050
  Std compression: 1.3923

Computing compression for layer 8 -> 9
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.5923
  Std compression: 1.3976

Computing compression for layer 9 -> 10
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -9.0127
  Std compression: 1.4084

Computing compression for layer 10 -> 11
Total tokens: 30303
Hidden dimension: 768
Finding 15 nearest neighbors...
Computing compression scores...
  Mean compression: -8.4687
  Std compression: 1.4923

Analyzing memorization-compression relationship...

Memorization Analysis (Layer 0):
  Correlation (point-biserial): r = -0.0328, p = 6.4494e-01
  Memorized sequences: mean = -5.4513, std = 0.2243, n = 100
  Novel sequences: mean = -5.4372, std = 0.2037, n = 100
  Difference: -0.0141
  T-test: t = -0.4615, p = 6.4493e-01
  ✗ WEAK: No significant correlation
Layer 0: r=-0.033, p=6.45e-01

Memorization Analysis (Layer 1):
  Correlation (point-biserial): r = -0.0544, p = 4.4409e-01
  Memorized sequences: mean = -6.0375, std = 0.1956, n = 100
  Novel sequences: mean = -6.0176, std = 0.1678, n = 100
  Difference: -0.0199
  T-test: t = -0.7669, p = 4.4407e-01
  ✗ WEAK: No significant correlation
Layer 1: r=-0.054, p=4.44e-01

Memorization Analysis (Layer 2):
  Correlation (point-biserial): r = -0.0684, p = 3.3566e-01
  Memorized sequences: mean = -6.5193, std = 0.2015, n = 100
  Novel sequences: mean = -6.4937, std = 0.1697, n = 100
  Difference: -0.0256
  T-test: t = -0.9651, p = 3.3565e-01
  ✗ WEAK: No significant correlation
Layer 2: r=-0.068, p=3.36e-01

Memorization Analysis (Layer 3):
  Correlation (point-biserial): r = -0.0684, p = 3.3618e-01
  Memorized sequences: mean = -6.8935, std = 0.2126, n = 100
  Novel sequences: mean = -6.8675, std = 0.1643, n = 100
  Difference: -0.0260
  T-test: t = -0.9641, p = 3.3618e-01
  ✗ WEAK: No significant correlation
Layer 3: r=-0.068, p=3.36e-01

Memorization Analysis (Layer 4):
  Correlation (point-biserial): r = -0.0553, p = 4.3684e-01
  Memorized sequences: mean = -7.2572, std = 0.2103, n = 100
  Novel sequences: mean = -7.2368, std = 0.1525, n = 100
  Difference: -0.0203
  T-test: t = -0.7791, p = 4.3686e-01
  ✗ WEAK: No significant correlation
Layer 4: r=-0.055, p=4.37e-01

Memorization Analysis (Layer 5):
  Correlation (point-biserial): r = -0.0895, p = 2.0741e-01
  Memorized sequences: mean = -7.5327, std = 0.2030, n = 100
  Novel sequences: mean = -7.5003, std = 0.1535, n = 100
  Difference: -0.0324
  T-test: t = -1.2649, p = 2.0740e-01
  ✗ WEAK: No significant correlation
Layer 5: r=-0.090, p=2.07e-01

Memorization Analysis (Layer 6):
  Correlation (point-biserial): r = -0.0901, p = 2.0473e-01
  Memorized sequences: mean = -7.8519, std = 0.2123, n = 100
  Novel sequences: mean = -7.8186, std = 0.1501, n = 100
  Difference: -0.0333
  T-test: t = -1.2724, p = 2.0473e-01
  ✗ WEAK: No significant correlation
Layer 6: r=-0.090, p=2.05e-01

Memorization Analysis (Layer 7):
  Correlation (point-biserial): r = -0.1441, p = 4.1780e-02
  Memorized sequences: mean = -8.2076, std = 0.2068, n = 100
  Novel sequences: mean = -8.1546, std = 0.1529, n = 100
  Difference: -0.0530
  T-test: t = -2.0490, p = 4.1782e-02
  ✗ WEAK: No significant correlation
Layer 7: r=-0.144, p=4.18e-02

Memorization Analysis (Layer 8):
  Correlation (point-biserial): r = -0.1617, p = 2.2150e-02
  Memorized sequences: mean = -8.5978, std = 0.2138, n = 100
  Novel sequences: mean = -8.5363, std = 0.1576, n = 100
  Difference: -0.0615
  T-test: t = -2.3060, p = 2.2147e-02
  ✗ WEAK: No significant correlation
Layer 8: r=-0.162, p=2.22e-02

Memorization Analysis (Layer 9):
  Correlation (point-biserial): r = -0.1943, p = 5.8438e-03
  Memorized sequences: mean = -9.0248, std = 0.2144, n = 100
  Novel sequences: mean = -8.9512, std = 0.1522, n = 100
  Difference: -0.0736
  T-test: t = -2.7867, p = 5.8436e-03
  ✗ WEAK: No significant correlation
Layer 9: r=-0.194, p=5.84e-03

Memorization Analysis (Layer 10):
  Correlation (point-biserial): r = -0.0602, p = 3.9711e-01
  Memorized sequences: mean = -8.4671, std = 0.2000, n = 100
  Novel sequences: mean = -8.4453, std = 0.1604, n = 100
  Difference: -0.0219
  T-test: t = -0.8487, p = 3.9709e-01
  ✗ WEAK: No significant correlation
Layer 10: r=-0.060, p=3.97e-01

Saved intermediate results to dynamics_results_standard/training_dynamics_results.pkl

Saved summary to dynamics_results_standard/training_summary.csv

======================================================================
EXPERIMENT COMPLETE
======================================================================
Results saved to: dynamics_results_standard
Total checkpoints: 8
Summary CSV: dynamics_results_standard/training_summary.csv

To visualize results, run:
  python scripts/visualize_training_dynamics.py --results_file dynamics_results_standard/training_dynamics_results.pkl
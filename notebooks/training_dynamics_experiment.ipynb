{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3624745",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('basin-compression-analysis'):\n",
    "    !git clone https://github.com/jacobposchl/basin-compression-analysis.git\n",
    "    os.chdir('basin-compression-analysis')\n",
    "else:\n",
    "    os.chdir('basin-compression-analysis')\n",
    "    !git pull\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cfefd",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust parameters below based on your computational budget:\n",
    "\n",
    "| Parameter | Quick Test | Standard | Deep Analysis |\n",
    "|-----------|------------|----------|---------------|\n",
    "| `num_passages` | 50 | 100 | 200 |\n",
    "| `epoch_schedule` | [1, 5, 20] | [1, 3, 5, 10, 20, 30, 50, 100] | [1, 3, 5, 7, 10, 15, 20, 30, 40, 50, 75, 100, 150, 200] |\n",
    "| Estimated time | ~1 hour | ~5-6 hours | ~12-15 hours |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a509a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "# Choose your experiment scale:\n",
    "\n",
    "# Quick test (1 hour on A100)\n",
    "# EXPERIMENT_MODE = \"quick\"\n",
    "# num_passages = 50\n",
    "# epoch_schedule = \"1,5,20\"\n",
    "\n",
    "# Standard experiment (5-6 hours on A100) - RECOMMENDED\n",
    "EXPERIMENT_MODE = \"standard\"\n",
    "num_passages = 100\n",
    "epoch_schedule = \"1,3,5,10,20,30,50,100\"\n",
    "\n",
    "# Deep analysis (12-15 hours on A100)\n",
    "# EXPERIMENT_MODE = \"deep\"\n",
    "# num_passages = 200\n",
    "# epoch_schedule = \"1,3,5,7,10,15,20,30,40,50,75,100,150,200\"\n",
    "\n",
    "# Other parameters\n",
    "model_name = \"gpt2\"\n",
    "learning_rate = 5e-5\n",
    "batch_size = 4\n",
    "output_dir = f\"dynamics_results_{EXPERIMENT_MODE}\"\n",
    "\n",
    "print(f\"Experiment Mode: {EXPERIMENT_MODE}\")\n",
    "print(f\"Number of passages: {num_passages}\")\n",
    "print(f\"Epoch schedule: {epoch_schedule}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79d80e",
   "metadata": {},
   "source": [
    "## Run Experiment\n",
    "\n",
    "This will train the model at each epoch count and compute compression + memorization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full experiment\n",
    "!python scripts/run_training_dynamics.py \\\n",
    "    --model {model_name} \\\n",
    "    --num_passages {num_passages} \\\n",
    "    --epoch_schedule {epoch_schedule} \\\n",
    "    --learning_rate {learning_rate} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --k_neighbors 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7ae37",
   "metadata": {},
   "source": [
    "## Quick Progress Check\n",
    "\n",
    "Check the summary CSV to see progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "summary_file = Path(output_dir) / 'training_summary.csv'\n",
    "\n",
    "if summary_file.exists():\n",
    "    df = pd.read_csv(summary_file)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROGRESS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot memorization progress\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Memorization rate\n",
    "    ax = axes[0]\n",
    "    ax.plot(df['epoch'], df['memorization_rate'] * 100, 'o-', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Memorization Rate (%)')\n",
    "    ax.set_title('Memorization Rate vs. Training Epochs')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best correlation\n",
    "    ax = axes[1]\n",
    "    ax.plot(df['epoch'], df['best_correlation'].abs(), 's-', linewidth=2, markersize=8, color='orange')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Absolute Correlation')\n",
    "    ax.set_title('Compression-Memorization Correlation vs. Training Epochs')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Summary file not found: {summary_file}\")\n",
    "    print(\"The experiment may not have started yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189437e",
   "metadata": {},
   "source": [
    "## Generate Visualizations\n",
    "\n",
    "Once the experiment completes, generate all analysis figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fab353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations\n",
    "!python scripts/visualize_training_dynamics.py \\\n",
    "    --results_file {output_dir}/training_dynamics_results.pkl \\\n",
    "    --output_dir {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670377a1",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea266e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary report\n",
    "report_file = Path(output_dir) / 'summary_report.txt'\n",
    "\n",
    "if report_file.exists():\n",
    "    with open(report_file, 'r') as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(f\"Report not found: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visualizations\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figures_dir = Path(output_dir) / 'figures'\n",
    "\n",
    "if figures_dir.exists():\n",
    "    figure_files = [\n",
    "        'compression_trajectories.png',\n",
    "        'memorization_vs_compression.png',\n",
    "        'layer_epoch_heatmap.png',\n",
    "        'individual_trajectories.png',\n",
    "        'memorization_rate.png'\n",
    "    ]\n",
    "    \n",
    "    for fig_name in figure_files:\n",
    "        fig_path = figures_dir / fig_name\n",
    "        if fig_path.exists():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"  {fig_name.replace('_', ' ').title()}\")\n",
    "            print('='*70)\n",
    "            display(Image(filename=str(fig_path)))\n",
    "        else:\n",
    "            print(f\"Figure not found: {fig_path}\")\n",
    "else:\n",
    "    print(f\"Figures directory not found: {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260c66c",
   "metadata": {},
   "source": [
    "## Advanced Analysis\n",
    "\n",
    "Load the results and perform custom analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from compression_lm.analysis.training_dynamics import (\n",
    "    analyze_u_shape_trajectory,\n",
    "    analyze_memorization_onset,\n",
    "    analyze_layer_temporal_patterns,\n",
    "    compute_compression_velocity\n",
    ")\n",
    "\n",
    "# Load results\n",
    "results_file = Path(output_dir) / 'training_dynamics_results.pkl'\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    all_results = data['all_results']\n",
    "    epochs = sorted(all_results.keys())\n",
    "    \n",
    "    print(f\"Loaded results for {len(epochs)} checkpoints\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "else:\n",
    "    print(f\"Results file not found: {results_file}\")\n",
    "    all_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze U-shape for each layer\n",
    "if all_results:\n",
    "    print(\"\\nU-SHAPE ANALYSIS BY LAYER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    num_layers = len(all_results[epochs[0]]['layer_analyses'])\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        u_analysis = analyze_u_shape_trajectory(all_results, layer_idx)\n",
    "        \n",
    "        print(f\"\\nLayer {layer_idx}:\")\n",
    "        print(f\"  Shape: {u_analysis['shape']}\")\n",
    "        print(f\"  R²: {u_analysis['r_squared']:.4f}\")\n",
    "        if u_analysis['vertex_epoch']:\n",
    "            print(f\"  Vertex at epoch: {u_analysis['vertex_epoch']:.1f}\")\n",
    "        print(f\"  Final compression difference: {u_analysis['final_diff']:.4f}\")\n",
    "        print(f\"  Trend: {u_analysis['trend']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa126fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memorization onset\n",
    "if all_results:\n",
    "    print(\"\\nMEMORIZATION ONSET ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    mid_layer = len(all_results[epochs[0]]['layer_analyses']) // 2\n",
    "    onset_analysis = analyze_memorization_onset(all_results, mid_layer)\n",
    "    \n",
    "    print(f\"\\nLayer {mid_layer} (middle layer):\")\n",
    "    print(f\"  Passages memorized: {onset_analysis['passages_memorized']}\")\n",
    "    print(f\"  Passages never memorized: {onset_analysis['passages_never_memorized']}\")\n",
    "    \n",
    "    if not np.isnan(onset_analysis['correlation']):\n",
    "        print(f\"  Correlation (initial compression vs onset epoch): r={onset_analysis['correlation']:.3f}\")\n",
    "        print(f\"  P-value: {onset_analysis['p_value']:.2e}\")\n",
    "        print(f\"  Mean onset epoch: {onset_analysis['mean_onset_epoch']:.1f}\")\n",
    "        print(f\"  Median onset epoch: {onset_analysis['median_onset_epoch']:.1f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if onset_analysis['correlation'] < -0.3 and onset_analysis['p_value'] < 0.01:\n",
    "            print(\"\\n  ✓ FINDING: Passages with lower initial compression memorize EARLIER\")\n",
    "            print(\"    This suggests compression predicts memorization difficulty.\")\n",
    "        elif onset_analysis['correlation'] > 0.3 and onset_analysis['p_value'] < 0.01:\n",
    "            print(\"\\n  ✓ FINDING: Passages with higher initial compression memorize EARLIER\")\n",
    "            print(\"    This suggests already-distinct patterns are easier to memorize.\")\n",
    "    else:\n",
    "        print(\"  Insufficient memorization to compute correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c739ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze layer temporal patterns\n",
    "if all_results:\n",
    "    print(\"\\nLAYER TEMPORAL PATTERNS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    temporal = analyze_layer_temporal_patterns(all_results)\n",
    "    \n",
    "    print(f\"\\nPattern: {temporal['early_vs_late']}\")\n",
    "    print(f\"\\nOrder of emergence (by significance):\")\n",
    "    for layer, epoch in temporal['layer_ordering_significance'][:5]:\n",
    "        print(f\"  Layer {layer}: epoch {epoch}\")\n",
    "    \n",
    "    print(f\"\\nOrder of emergence (by effect size):\")\n",
    "    for layer, epoch in temporal['layer_ordering_effect_size'][:5]:\n",
    "        print(f\"  Layer {layer}: epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity analysis for middle layer\n",
    "if all_results:\n",
    "    mid_layer = len(all_results[epochs[0]]['layer_analyses']) // 2\n",
    "    velocity_data = compute_compression_velocity(all_results, mid_layer)\n",
    "    \n",
    "    print(f\"\\nVELOCITY ANALYSIS (Layer {mid_layer})\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Max velocity: {velocity_data['max_velocity']:.4f}\")\n",
    "    print(f\"Max acceleration: {velocity_data['max_acceleration']:.4f}\")\n",
    "    print(f\"Velocity sign changes: {velocity_data['velocity_sign_changes']}\")\n",
    "    \n",
    "    # Plot\n",
    "    from compression_lm.analysis.dynamics_visualizations import plot_compression_velocity\n",
    "    \n",
    "    fig = plot_compression_velocity(all_results, mid_layer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a477d",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Download the results to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip results for download\n",
    "import shutil\n",
    "\n",
    "zip_file = f\"{output_dir}.zip\"\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "\n",
    "print(f\"\\nResults zipped to: {zip_file}\")\n",
    "print(f\"File size: {os.path.getsize(zip_file) / 1e6:.1f} MB\")\n",
    "print(\"\\nTo download in Colab:\")\n",
    "print(f\"  from google.colab import files\")\n",
    "print(f\"  files.download('{zip_file}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe62d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download via Colab\n",
    "from google.colab import files\n",
    "files.download(f\"{output_dir}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3745e",
   "metadata": {},
   "source": [
    "## Interpretation Guide\n",
    "\n",
    "### Scenario 1: U-Shaped Curve ✓\n",
    "**Pattern:** Compression decreases → plateaus → increases  \n",
    "**Interpretation:** Model goes through learning → consolidation → memorization phases  \n",
    "**Scientific Value:** HIGH - Novel mechanistic finding about geometric phase transitions\n",
    "\n",
    "### Scenario 2: Monotonic Decrease\n",
    "**Pattern:** Compression steadily decreases across all epochs  \n",
    "**Interpretation:** LLM memorization differs from VAE compression; maintains distinctions even for memorized content  \n",
    "**Scientific Value:** MEDIUM-HIGH - Important difference between reconstruction vs. generation\n",
    "\n",
    "### Scenario 3: Layer-Dependent Patterns\n",
    "**Pattern:** Early layers increase, late layers decrease (or vice versa)  \n",
    "**Interpretation:** Hierarchical organization of memorization mechanisms  \n",
    "**Scientific Value:** HIGH - Reveals layer-wise specialization\n",
    "\n",
    "### Scenario 4: No Memorization Achieved\n",
    "**Pattern:** Reproduction accuracy stays low even with 100 epochs  \n",
    "**Next Step:** Run deeper experiment with fewer passages (10 passages, 200 epochs)\n",
    "\n",
    "### Scenario 5: Passage Heterogeneity\n",
    "**Pattern:** Different passages show different compression patterns  \n",
    "**Interpretation:** Memorization mechanisms vary by content type  \n",
    "**Next Step:** Categorize passages by properties (length, complexity, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907d263",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you use this experiment in your research:\n",
    "\n",
    "```\n",
    "@article{basin-compression-analysis,\n",
    "  title={Compression Dynamics in Language Model Memorization},\n",
    "  author={Your Name},\n",
    "  year={2025},\n",
    "  url={https://github.com/jacobposchl/basin-compression-analysis}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

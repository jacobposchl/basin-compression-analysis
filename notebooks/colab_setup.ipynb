{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compression Basins in Language Models - Colab Setup\n",
        "\n",
        "This notebook sets up and runs the compression basin experiments in Google Colab.\n",
        "\n",
        "**Important**: Enable GPU in Colab: Runtime > Change runtime type > GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers numpy scipy scikit-learn faiss-cpu nltk spacy matplotlib seaborn pandas tqdm datasets statsmodels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "print(\"NLTK data downloaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Repository\n",
        "\n",
        "**Replace `YOUR_USERNAME` with your GitHub username**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your GitHub repository URL\n",
        "!git clone https://github.com/YOUR_USERNAME/basin-compression-analysis.git\n",
        "%cd basin-compression-analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the package in development mode\n",
        "!pip install -e .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Verify GPU Availability\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Warning: No GPU detected. Experiments will run on CPU (much slower).\")\n",
        "    print(\"To enable GPU: Runtime > Change runtime type > GPU\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Experiments\n",
        "\n",
        "Choose one of the experiments below:\n",
        "\n",
        "**Note**: The examples use `--use_small_dataset` flag which uses WikiText-2 (smaller, ~4MB) instead of WikiText-103 (larger, ~300MB) for faster testing. Remove this flag for the full dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Memorization Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use --use_small_dataset for faster testing (WikiText-2 instead of WikiText-103)\n",
        "# Remove this flag for the full dataset\n",
        "!python scripts/run_memorization.py --max_sequences 100 --k_neighbors 15 --max_length 128 --use_small_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Token Importance Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use --use_small_dataset for faster testing\n",
        "!python scripts/run_token_importance.py --max_sequences 50 --k_neighbors 15 --max_length 128 --use_small_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option C: Linguistic Structure Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use --use_small_dataset for faster testing\n",
        "!python scripts/run_linguistic.py --max_sequences 100 --k_neighbors 15 --max_length 128 --use_small_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option D: Full Pipeline (All Experiments)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use --use_small_dataset for faster testing\n",
        "!python scripts/run_full_pipeline.py --max_sequences 100 --k_neighbors 15 --max_length 128 --use_small_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: View Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display images inline\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "# Display any generated plots\n",
        "for img_file in glob.glob(\"*.png\"):\n",
        "    print(f\"\\n{img_file}:\")\n",
        "    display(Image(img_file))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Use as Python Library\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Run memorization experiment programmatically\n",
        "from compression_lm.models.model_loader import load_model\n",
        "from compression_lm.data.load_datasets import load_wikitext\n",
        "from compression_lm.experiments.memorization import run_memorization_experiment\n",
        "\n",
        "# Load model (automatically uses GPU if available)\n",
        "model, tokenizer, device = load_model('gpt2')\n",
        "\n",
        "# Load data\n",
        "texts = load_wikitext(split='test', max_samples=50)\n",
        "\n",
        "# Run experiment\n",
        "results = run_memorization_experiment(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    texts=texts,\n",
        "    k_neighbors=15,\n",
        "    max_sequences=50,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "print(\"\\nExperiment complete!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
